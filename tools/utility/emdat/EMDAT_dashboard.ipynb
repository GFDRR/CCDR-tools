{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "381be633-14f7-40a4-b9f1-03a7fcc57867",
   "metadata": {},
   "source": [
    "# EMDAT Disaster Data Dashboard - Enhanced with Leafmap\n",
    "## Interactive Dashboard with Quantile-Based Choropleth Maps\n",
    "\n",
    "This dashboard provides comprehensive analysis of EMDAT disaster data with enhanced subnational visualization using Leafmap for superior legend control and quantile-based classification.\n",
    "\n",
    "### Features:\n",
    "- **Country Selection**: Choose specific countries or view global data, administrative levels: ADM1 and ADM2\n",
    "- **Time Range Filtering**: Adjust the year range using the slider\n",
    "- **Multiple Visualization Tabs**: Overview, disaster types, temporal trends, loss analysis, and subnational analysis\n",
    "- **Choropleth Maps**: Quantile-based classification (5 classes) for multiple metrics: Deaths, Affected Population, Disaster Count.\n",
    "\n",
    "### Required input:\n",
    "1. **EMDAT database**: download the latest global dataset of natural disasters from [**EMDAT**](https://www.emdat.be/) or use this export: [**emdat_2025.xlsx**](https://github.com/GFDRR/CCDR-tools/raw/refs/heads/main/tools/utility/emdat/emdat_2025.xlsx) \n",
    "2. **ADM units boundaries**: EMDAT geomapping is based on [**GAUL 2015**](https://data.amerigeoss.org/it/dataset/global-administrative-boundaries-regions-gaul-2015) dataset. A refined version of the dataset produced for this tool is available here: [**ADM_GAUL.gpkg**](https://github.com/GFDRR/CCDR-tools/raw/refs/heads/main/tools/utility/emdat/ADM_GAUL.gpkg) \n",
    "\n",
    "### Usage Tips:\n",
    "1. **For best choropleth results**: Select countries with high subnational data coverage\n",
    "2. **Legend management**: Toggle layers on/off to show only relevant legends\n",
    "3. **Interactive maps**: Click on administrative units for detailed information\n",
    "4. **Performance**: Large datasets may take a few seconds to render choropleth maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f2d0e68-6076-4d0b-8797-a3ccfe3d4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIGURATION PARAMETERS\n",
    "EXCEL_FILE_PATH = 'emdat_2025.xlsx'  # Set your EMDAT Excel file path here\n",
    "GPKG_FILE_PATH = 'X:/Work/Geodata/ADM/ADM_GAUL.gpkg'  # Set your ADM units GPKG file path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d62a1518-9e90-4b13-9e5b-e2a6c54e8e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GeoPandas available - choropleth maps enabled\n",
      "üöÄ INITIALIZING EMDAT DASHBOARD WITH FIXED CHOROPLETH\n",
      "============================================================\n",
      "\n",
      "üìÅ Checking file availability...\n",
      "‚úÖ EMDAT Excel file found: emdat_2025.xlsx\n",
      "‚úÖ GPKG file found: X:/Work/Geodata/ADM/ADM_GAUL.gpkg\n",
      "\n",
      "üèóÔ∏è Initializing EMDAT Dashboard...\n",
      "Loading data from emdat_2025.xlsx...\n",
      "Data loaded successfully!\n",
      "Shape: (15739, 46)\n",
      "Years covered: 1900 - 2025\n",
      "Countries: 228\n",
      "Disaster types: 10\n",
      "Data preparation completed!\n",
      "\n",
      "Checking subnational data availability...\n",
      "Countries with Subnational Data:\n",
      "==================================================\n",
      "Saint Helena (SHN): 100.0% coverage (1/1 disasters)\n",
      "Northern Mariana Islands (MNP): 100.0% coverage (5/5 disasters)\n",
      "Timor-Leste (TLS): 100.0% coverage (10/10 disasters)\n",
      "Cayman Islands (CYM): 100.0% coverage (7/7 disasters)\n",
      "Qatar (QAT): 100.0% coverage (1/1 disasters)\n",
      "South Sudan (SSD): 95.0% coverage (19/20 disasters)\n",
      "North Macedonia (MKD): 91.3% coverage (21/23 disasters)\n",
      "Burundi (BDI): 90.2% coverage (46/51 disasters)\n",
      "State of Palestine (PSE): 87.5% coverage (7/8 disasters)\n",
      "Angola (AGO): 85.7% coverage (48/56 disasters)\n",
      "Saudi Arabia (SAU): 85.2% coverage (23/27 disasters)\n",
      "Bosnia and Herzegovina (BIH): 83.9% coverage (26/31 disasters)\n",
      "Croatia (HRV): 82.9% coverage (29/35 disasters)\n",
      "Serbia (SRB): 81.8% coverage (27/33 disasters)\n",
      "Namibia (NAM): 81.5% coverage (22/27 disasters)\n",
      "Rwanda (RWA): 80.0% coverage (36/45 disasters)\n",
      "Seychelles (SYC): 80.0% coverage (4/5 disasters)\n",
      "Suriname (SUR): 80.0% coverage (4/5 disasters)\n",
      "Serbia Montenegro (SCG): 78.6% coverage (11/14 disasters)\n",
      "Tajikistan (TJK): 78.3% coverage (54/69 disasters)\n",
      "============================================================\n",
      "EMDAT DISASTER DATA DASHBOARD - FIXED CHOROPLETH\n",
      "============================================================\n",
      "\n",
      "Initializing dashboard components...\n",
      "‚úÖ GPKG boundaries available: X:/Work/Geodata/ADM/ADM_GAUL.gpkg\n",
      "üó∫Ô∏è Enhanced choropleth mapping enabled!\n",
      "\n",
      "üìä Data Quality Report:\n",
      "  - Total records: 15,739\n",
      "  - Missing values in key columns:\n",
      "    ‚Ä¢ Total Deaths: 0 (0.0%)\n",
      "    ‚Ä¢ Total Affected: 0 (0.0%)\n",
      "    ‚Ä¢ Total Damages ('000 US$): 0 (0.0%)\n",
      "\n",
      "üèõÔ∏è Subnational Data Availability:\n",
      "  - Records with admin units: 8,428 (53.5%)\n",
      "  - Top countries for choropleth mapping:\n",
      "    ‚Ä¢ Guatemala (GTM): 63.9% coverage\n",
      "    ‚Ä¢ Myanmar (MMR): 55.7% coverage\n",
      "    ‚Ä¢ India (IND): 49.9% coverage\n",
      "    ‚Ä¢ United States of America (USA): 48.4% coverage\n",
      "    ‚Ä¢ Saint Vincent and the Grenadines (VCT): 44.0% coverage\n",
      "\n",
      "üåç Dataset Summary:\n",
      "  - Countries: 228\n",
      "  - Time period: 1900-2025\n",
      "  - Disaster types: 10\n",
      "  - Total deaths: 23,058,268\n",
      "  - Total affected: 8,810,390,058\n",
      "\n",
      "üöÄ Dashboard Features:\n",
      "  ‚úÖ Interactive country and time filtering\n",
      "  ‚úÖ Multiple visualization tabs\n",
      "  ‚úÖ Statistical analysis and charts\n",
      "  ‚úÖ FIXED multi-layer choropleth maps with:\n",
      "      ‚Ä¢ 6 independent layers (ADM1/ADM2 √ó 3 metrics)\n",
      "      ‚Ä¢ Proper quantile-based color scaling\n",
      "      ‚Ä¢ 250px left-side legends with visibility control\n",
      "      ‚Ä¢ Layer checkboxes for independent toggling\n",
      "      ‚Ä¢ Hover tooltips and administrative unit info\n",
      "\n",
      "‚úÖ Dashboard ready! Use the interactive controls to explore the data.\n",
      "------------------------------------------------------------\n",
      "\n",
      "üéâ STARTING INTERACTIVE DASHBOARD\n",
      "==================================================\n",
      "üéØ Select a country with subnational data to see enhanced choropleth maps!\n",
      "üìä Use the controls below to filter and explore the data.\n",
      "\n",
      "üí° Pro tip: Countries with higher subnational coverage will have better choropleth maps.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f0bbd27579448a8468bb9b6cd49d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>EMDAT Disaster Data Dashboard - Fixed Choropleth</h1>'), HTML(value=\"\\n    <div‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import folium\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import json\n",
    "import warnings\n",
    "import re\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optional: For advanced mapping\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    GEOPANDAS_AVAILABLE = True\n",
    "    print(\"‚úÖ GeoPandas available - choropleth maps enabled\")\n",
    "except ImportError:\n",
    "    GEOPANDAS_AVAILABLE = False\n",
    "    print(\"‚ùå GeoPandas not available - install with: pip install geopandas\")\n",
    "\n",
    "try:\n",
    "    from scipy import stats\n",
    "    SCIPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SCIPY_AVAILABLE = False\n",
    "    print(\"SciPy not installed. Some trend analysis features will be limited.\")\n",
    "\n",
    "class EMDATDashboard:\n",
    "    \"\"\"Main class for EMDAT disaster data dashboard\"\"\"\n",
    "    \n",
    "    def __init__(self, excel_file_path):\n",
    "        \"\"\"Initialize dashboard with EMDAT Excel file\"\"\"\n",
    "        self.file_path = excel_file_path\n",
    "        self.df = None\n",
    "        self.selected_country = None\n",
    "        self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load EMDAT data from Excel file\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading data from {self.file_path}...\")\n",
    "            self.df = pd.read_excel(self.file_path)\n",
    "            \n",
    "            print(f\"Data loaded successfully!\")\n",
    "            print(f\"Shape: {self.df.shape}\")\n",
    "            print(f\"Years covered: {self.df['Start Year'].min()} - {self.df['Start Year'].max()}\")\n",
    "            print(f\"Countries: {self.df['ISO'].nunique()}\")\n",
    "            print(f\"Disaster types: {self.df['Disaster Type'].nunique()}\")\n",
    "            \n",
    "            self.prepare_data()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            print(\"Please ensure your EMDAT Excel file is in the correct location\")\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Clean and prepare the data for analysis\"\"\"\n",
    "        # Handle missing values\n",
    "        numeric_columns = ['Total Deaths', 'Total Affected', 'No Injured', \n",
    "                          'No Affected', 'No Homeless', 'Total Damages (\\'000 US$)',\n",
    "                          'Insured Damages (\\'000 US$)', 'Reconstruction Costs (\\'000 US$)']\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
    "                self.df[col] = self.df[col].fillna(0)\n",
    "        \n",
    "        # Create decade column for temporal analysis\n",
    "        self.df['Decade'] = (self.df['Start Year'] // 10) * 10\n",
    "        \n",
    "        # Create simplified disaster categories\n",
    "        self.df['Disaster Category'] = self.df['Disaster Type'].apply(self.categorize_disaster)\n",
    "        \n",
    "        # Calculate total losses (deaths + affected)\n",
    "        self.df['Total Losses'] = self.df['Total Deaths'] + self.df['Total Affected']\n",
    "        \n",
    "        print(\"Data preparation completed!\")\n",
    "    \n",
    "    def categorize_disaster(self, disaster_type):\n",
    "        \"\"\"Categorize disasters into broader groups\"\"\"\n",
    "        if pd.isna(disaster_type):\n",
    "            return 'Other'\n",
    "        \n",
    "        disaster_type = str(disaster_type).lower()\n",
    "        \n",
    "        if any(word in disaster_type for word in ['flood', 'storm', 'cyclone', 'hurricane', 'typhoon']):\n",
    "            return 'Hydrometeorological'\n",
    "        elif any(word in disaster_type for word in ['earthquake', 'volcanic', 'landslide']):\n",
    "            return 'Geophysical'\n",
    "        elif any(word in disaster_type for word in ['drought', 'extreme temperature', 'wildfire']):\n",
    "            return 'Climatological'\n",
    "        elif any(word in disaster_type for word in ['epidemic', 'infestation']):\n",
    "            return 'Biological'\n",
    "        else:\n",
    "            return 'Other'\n",
    "\n",
    "def parse_admin_units(admin_units_str):\n",
    "    \"\"\"Parse the JSON-formatted Admin Units string\"\"\"\n",
    "    if pd.isna(admin_units_str) or admin_units_str == '':\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        admin_str = str(admin_units_str).strip()\n",
    "        \n",
    "        if admin_str.startswith('[') and admin_str.endswith(']'):\n",
    "            admin_data = json.loads(admin_str)\n",
    "            if isinstance(admin_data, list):\n",
    "                return admin_data\n",
    "            else:\n",
    "                return [admin_data]\n",
    "        elif admin_str.startswith('{') and admin_str.endswith('}'):\n",
    "            admin_data = json.loads(admin_str)\n",
    "            return [admin_data]\n",
    "        else:\n",
    "            admin_data = json.loads(admin_str)\n",
    "            return admin_data if isinstance(admin_data, list) else [admin_data]\n",
    "            \n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        # Fallback to regex parsing\n",
    "        try:\n",
    "            adm1_matches = re.findall(r'\"adm1_name\"\\s*:\\s*\"([^\"]+)\"', str(admin_units_str))\n",
    "            adm1_codes = re.findall(r'\"adm1_code\"\\s*:\\s*(\\d+)', str(admin_units_str))\n",
    "            adm2_matches = re.findall(r'\"adm2_name\"\\s*:\\s*\"([^\"]+)\"', str(admin_units_str))\n",
    "            adm2_codes = re.findall(r'\"adm2_code\"\\s*:\\s*(\\d+)', str(admin_units_str))\n",
    "            \n",
    "            result = []\n",
    "            \n",
    "            for i, name in enumerate(adm1_matches):\n",
    "                code = adm1_codes[i] if i < len(adm1_codes) else None\n",
    "                result.append({\n",
    "                    \"adm1_name\": name,\n",
    "                    \"adm1_code\": int(code) if code else None\n",
    "                })\n",
    "            \n",
    "            for i, name in enumerate(adm2_matches):\n",
    "                code = adm2_codes[i] if i < len(adm2_codes) else None\n",
    "                result.append({\n",
    "                    \"adm2_name\": name,\n",
    "                    \"adm2_code\": int(code) if code else None\n",
    "                })\n",
    "            \n",
    "            return result if result else []\n",
    "                \n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "# ============================================================================\n",
    "# DYNAMIC COUNTRY MAPPINGS FROM BOUNDARY FILE\n",
    "# ============================================================================\n",
    "\n",
    "# Global cache for country mappings to avoid reading file multiple times\n",
    "_country_mappings_cache = {}\n",
    "\n",
    "def build_country_mappings_from_boundaries(gpkg_file_path):\n",
    "    \"\"\"\n",
    "    Build dynamic country mappings from the ADM_0 layer in the boundary file.\n",
    "    Returns iso_to_gaul and iso_to_names mappings extracted from the data.\n",
    "    \"\"\"\n",
    "    global _country_mappings_cache\n",
    "    \n",
    "    # Check if mappings are already cached\n",
    "    if gpkg_file_path in _country_mappings_cache:\n",
    "        return _country_mappings_cache[gpkg_file_path]\n",
    "    \n",
    "    try:\n",
    "        print(f\"  üìö Building dynamic country mappings from {gpkg_file_path}...\")\n",
    "        \n",
    "        # Check if ADM_0 layer exists\n",
    "        available_layers = gpd.list_layers(gpkg_file_path)\n",
    "        if hasattr(available_layers, 'name'):\n",
    "            layer_names = available_layers['name'].tolist()\n",
    "        else:\n",
    "            layer_names = available_layers\n",
    "        \n",
    "        # Try different possible ADM_0 layer names\n",
    "        adm0_layer_names = ['ADM_0', 'ADM0', 'admin_0', 'level_0', 'countries']\n",
    "        adm0_layer = None\n",
    "        \n",
    "        for layer_name in adm0_layer_names:\n",
    "            if layer_name in layer_names:\n",
    "                adm0_layer = layer_name\n",
    "                break\n",
    "        \n",
    "        if not adm0_layer:\n",
    "            print(f\"    ‚ö†Ô∏è No ADM_0 layer found in available layers: {layer_names}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Read ADM_0 layer\n",
    "        print(f\"    üìÇ Reading {adm0_layer} layer...\")\n",
    "        adm0_gdf = gpd.read_file(gpkg_file_path, layer=adm0_layer)\n",
    "        print(f\"    üìä Loaded {len(adm0_gdf)} country records\")\n",
    "        \n",
    "        # Show available columns for debugging\n",
    "        print(f\"    üîç Available columns: {list(adm0_gdf.columns)}\")\n",
    "        \n",
    "        # Initialize mappings\n",
    "        iso_to_gaul = {}\n",
    "        iso_to_names = {}\n",
    "        \n",
    "        # Define possible column name variations\n",
    "        iso_columns = ['ISO3166_a3', 'ISO_a3', 'ISO3', 'ISO', 'ADM0_A3']\n",
    "        gaul_columns = ['ADM0_CODE', 'GAUL_CODE', 'ADM0_CD', 'GAUL']\n",
    "        name_columns = ['ADM0_NAME', 'NAME_EN', 'COUNTRY', 'NAME', 'ADM0_NAME_EN']\n",
    "        \n",
    "        # Find the actual column names\n",
    "        iso_col = None\n",
    "        gaul_col = None\n",
    "        name_col = None\n",
    "        \n",
    "        for col in iso_columns:\n",
    "            if col in adm0_gdf.columns:\n",
    "                iso_col = col\n",
    "                break\n",
    "        \n",
    "        for col in gaul_columns:\n",
    "            if col in adm0_gdf.columns:\n",
    "                gaul_col = col\n",
    "                break\n",
    "        \n",
    "        for col in name_columns:\n",
    "            if col in adm0_gdf.columns:\n",
    "                name_col = col\n",
    "                break\n",
    "        \n",
    "        print(f\"    üéØ Using columns - ISO: {iso_col}, GAUL: {gaul_col}, NAME: {name_col}\")\n",
    "        \n",
    "        if not iso_col:\n",
    "            print(f\"    ‚ùå No ISO column found in {iso_columns}\")\n",
    "            return None, None\n",
    "        \n",
    "        if not gaul_col:\n",
    "            print(f\"    ‚ùå No GAUL code column found in {gaul_columns}\")\n",
    "            return None, None\n",
    "        \n",
    "        if not name_col:\n",
    "            print(f\"    ‚ùå No name column found in {name_columns}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Build mappings from the data\n",
    "        mapping_count = 0\n",
    "        \n",
    "        for idx, row in adm0_gdf.iterrows():\n",
    "            iso_code = row.get(iso_col)\n",
    "            gaul_code = row.get(gaul_col)\n",
    "            country_name = row.get(name_col)\n",
    "            \n",
    "            # Skip rows with missing essential data\n",
    "            if pd.isna(iso_code) or pd.isna(gaul_code) or pd.isna(country_name):\n",
    "                continue\n",
    "            \n",
    "            # Clean and standardize the values\n",
    "            iso_code = str(iso_code).strip().upper()\n",
    "            gaul_code = int(gaul_code) if pd.notna(gaul_code) else None\n",
    "            country_name = str(country_name).strip()\n",
    "            \n",
    "            # Only process valid ISO codes (3 characters)\n",
    "            if len(iso_code) == 3 and gaul_code is not None:\n",
    "                iso_to_gaul[iso_code] = gaul_code\n",
    "                \n",
    "                # Store country name as a list for consistency with original structure\n",
    "                iso_to_names[iso_code] = [country_name]\n",
    "                mapping_count += 1\n",
    "        \n",
    "        print(f\"    ‚úÖ Built mappings for {mapping_count} countries\")\n",
    "        \n",
    "        # Cache the results\n",
    "        _country_mappings_cache[gpkg_file_path] = (iso_to_gaul, iso_to_names)\n",
    "        \n",
    "        # Show sample mappings for verification\n",
    "        sample_isos = list(iso_to_gaul.keys())[:5]\n",
    "        print(f\"    üìã Sample mappings:\")\n",
    "        for iso in sample_isos:\n",
    "            print(f\"      {iso}: GAUL={iso_to_gaul[iso]}, NAME={iso_to_names[iso][0]}\")\n",
    "        \n",
    "        return iso_to_gaul, iso_to_names\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ùå Error building country mappings: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def get_country_boundaries_robust(boundaries_gdf, country_iso, country_name, gpkg_file_path=None):\n",
    "    \"\"\"Get country boundaries using multiple fallback methods - DYNAMIC VERSION\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"  üîç Searching for {country_iso} boundaries using dynamic mappings...\")\n",
    "        \n",
    "        # Method 1: Try ISO_a3 field (most common)\n",
    "        if 'ISO_a3' in boundaries_gdf.columns:\n",
    "            print(f\"    üîç Trying ISO_a3 field...\")\n",
    "            country_boundaries = boundaries_gdf[boundaries_gdf['ISO_a3'] == country_iso].copy()\n",
    "            if not country_boundaries.empty:\n",
    "                print(f\"    ‚úÖ Found {len(country_boundaries)} boundaries using ISO_a3\")\n",
    "                return country_boundaries\n",
    "        \n",
    "        # Method 2: Try other ISO fields\n",
    "        for iso_field in ['ISO3166_a3', 'ADM0_A3', 'ISO', 'ISO3', 'COUNTRY_ISO']:\n",
    "            if iso_field in boundaries_gdf.columns:\n",
    "                print(f\"    üîç Trying {iso_field} field...\")\n",
    "                country_boundaries = boundaries_gdf[boundaries_gdf[iso_field].astype(str).str.upper() == country_iso.upper()].copy()\n",
    "                if not country_boundaries.empty:\n",
    "                    print(f\"    ‚úÖ Found {len(country_boundaries)} boundaries using {iso_field}\")\n",
    "                    return country_boundaries\n",
    "        \n",
    "        # Method 3: Try ADM0_NAME with DYNAMIC country mapping\n",
    "        if 'ADM0_NAME' in boundaries_gdf.columns and gpkg_file_path:\n",
    "            print(f\"    üîç Trying ADM0_NAME field with dynamic country name mapping...\")\n",
    "            \n",
    "            # Get dynamic mappings from the boundary file\n",
    "            iso_to_gaul, iso_to_names = build_country_mappings_from_boundaries(gpkg_file_path)\n",
    "            \n",
    "            if iso_to_names and country_iso in iso_to_names:\n",
    "                for country_name_variant in iso_to_names[country_iso]:\n",
    "                    country_boundaries = boundaries_gdf[boundaries_gdf['ADM0_NAME'].str.contains(country_name_variant, case=False, na=False)].copy()\n",
    "                    if not country_boundaries.empty:\n",
    "                        print(f\"    ‚úÖ Found {len(country_boundaries)} boundaries using ADM0_NAME: {country_name_variant}\")\n",
    "                        return country_boundaries\n",
    "            else:\n",
    "                print(f\"    ‚ö†Ô∏è Dynamic name mapping not available or {country_iso} not found\")\n",
    "        \n",
    "        # Method 4: Try GAUL codes with DYNAMIC mapping\n",
    "        if 'ADM0_CODE' in boundaries_gdf.columns and gpkg_file_path:\n",
    "            print(f\"    üîç Trying GAUL codes with dynamic mapping...\")\n",
    "            \n",
    "            # Get dynamic mappings from the boundary file  \n",
    "            iso_to_gaul, iso_to_names = build_country_mappings_from_boundaries(gpkg_file_path)\n",
    "            \n",
    "            if iso_to_gaul and country_iso in iso_to_gaul:\n",
    "                gaul_code = iso_to_gaul[country_iso]\n",
    "                country_boundaries = boundaries_gdf[boundaries_gdf['ADM0_CODE'] == gaul_code].copy()\n",
    "                if not country_boundaries.empty:\n",
    "                    print(f\"    ‚úÖ Found {len(country_boundaries)} boundaries using dynamic GAUL code: {gaul_code}\")\n",
    "                    return country_boundaries\n",
    "            else:\n",
    "                print(f\"    ‚ö†Ô∏è Dynamic GAUL mapping not available or {country_iso} not found\")\n",
    "        \n",
    "        print(f\"    ‚ùå No boundaries found for {country_iso} using any method\")\n",
    "        \n",
    "        # Debug: Show available country identifiers\n",
    "        print(f\"    üîç Debug - Available fields in GPKG:\")\n",
    "        for col in ['ADM0_NAME', 'ADM0_CODE', 'ADM0_A3', 'ISO_a3', 'ISO3166_a3', 'ISO', 'ISO3']:\n",
    "            if col in boundaries_gdf.columns:\n",
    "                sample_values = boundaries_gdf[col].dropna().unique()[:5]\n",
    "                print(f\"      {col}: {sample_values}\")\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ùå Error in dynamic boundary matching: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_merged_boundary_data(admin_data, level, country_iso, country_name, gpkg_file_path):\n",
    "    \"\"\"Create merged boundary data - DYNAMIC VERSION with improved country matching\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Determine GPKG layer and code column\n",
    "        if level == 'ADM1':\n",
    "            gpkg_layer = 'ADM_1'\n",
    "            code_col = 'ADM1_CODE'\n",
    "            name_col = 'ADM1_NAME'\n",
    "        elif level == 'ADM2':\n",
    "            gpkg_layer = 'ADM_2'\n",
    "            code_col = 'ADM2_CODE'\n",
    "            name_col = 'ADM2_NAME'\n",
    "        else:\n",
    "            print(f\"  ‚ùå Unsupported admin level: {level}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"  üìÇ Loading {gpkg_layer} boundaries for {country_iso}...\")\n",
    "        \n",
    "        # Check if the layer exists in the GPKG file\n",
    "        try:\n",
    "            available_layers = gpd.list_layers(gpkg_file_path)\n",
    "            if hasattr(available_layers, 'name'):\n",
    "                layer_names = available_layers['name'].tolist()\n",
    "            else:\n",
    "                layer_names = available_layers\n",
    "            \n",
    "            if gpkg_layer not in layer_names:\n",
    "                print(f\"  ‚ùå Layer {gpkg_layer} not found in GPKG\")\n",
    "                print(f\"  üìã Available layers: {layer_names}\")\n",
    "                # Try alternative layer names\n",
    "                alt_names = {\n",
    "                    'ADM_1': ['ADM1', 'admin_1', 'adm1', 'level_1'],\n",
    "                    'ADM_2': ['ADM2', 'admin_2', 'adm2', 'level_2']\n",
    "                }\n",
    "                \n",
    "                if gpkg_layer in alt_names:\n",
    "                    for alt_name in alt_names[gpkg_layer]:\n",
    "                        if alt_name in layer_names:\n",
    "                            print(f\"  üîÑ Using alternative layer name: {alt_name}\")\n",
    "                            gpkg_layer = alt_name\n",
    "                            break\n",
    "                    else:\n",
    "                        print(f\"  ‚ùå No suitable alternative layer found\")\n",
    "                        return None\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Could not check layer availability: {e}\")\n",
    "        \n",
    "        boundaries_gdf = gpd.read_file(gpkg_file_path, layer=gpkg_layer)\n",
    "        print(f\"  üìä Loaded {len(boundaries_gdf)} boundaries from {gpkg_layer}\")\n",
    "        \n",
    "        # Get country boundaries using DYNAMIC method\n",
    "        country_boundaries = get_country_boundaries_robust(boundaries_gdf, country_iso, country_name, gpkg_file_path)\n",
    "        \n",
    "        if country_boundaries is None or country_boundaries.empty:\n",
    "            print(f\"  ‚ùå No boundaries found for {country_iso} in {gpkg_layer}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"  ‚úÖ Found {len(country_boundaries)} {level} boundaries\")\n",
    "        \n",
    "        # Aggregate admin data\n",
    "        admin_summary = admin_data.groupby(['Admin Code', 'Admin Unit']).agg({\n",
    "            'Deaths': 'sum',\n",
    "            'Affected': 'sum',\n",
    "            'Damage (000 USD)': 'sum',\n",
    "            'Year': 'count'\n",
    "        }).reset_index()\n",
    "        admin_summary.columns = ['Admin Code', 'Admin Unit', 'Total Deaths', 'Total Affected', 'Total Damage', 'Disaster Count']\n",
    "        \n",
    "        # Remove null codes and convert to string\n",
    "        admin_summary = admin_summary[admin_summary['Admin Code'].notna()].copy()\n",
    "        admin_summary['Admin Code'] = admin_summary['Admin Code'].astype(str).str.strip()\n",
    "        country_boundaries[code_col] = country_boundaries[code_col].astype(str).str.strip()\n",
    "        \n",
    "        # Merge with boundaries\n",
    "        merged_data = country_boundaries.merge(\n",
    "            admin_summary,\n",
    "            left_on=code_col,\n",
    "            right_on='Admin Code',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Fill NaN values\n",
    "        for col in ['Total Deaths', 'Total Affected', 'Total Damage', 'Disaster Count']:\n",
    "            merged_data[col] = merged_data[col].fillna(0)\n",
    "        \n",
    "        # Add admin names from boundaries if missing\n",
    "        if name_col in merged_data.columns:\n",
    "            merged_data['Admin Unit'] = merged_data['Admin Unit'].fillna(merged_data[name_col])\n",
    "        \n",
    "        matched_count = len(merged_data[merged_data['Disaster Count'] > 0])\n",
    "        print(f\"  ‚úÖ Matched {matched_count}/{len(merged_data)} boundaries with disaster data\")\n",
    "        \n",
    "        return merged_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error creating merged boundary data: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# UPDATED CHOROPLETH FUNCTION WITH DYNAMIC MAPPINGS\n",
    "# ============================================================================\n",
    "\n",
    "def create_working_multi_layer_choropleth(admin_df, original_df, gpkg_file_path):\n",
    "    \"\"\"\n",
    "    DYNAMIC VERSION: Create a working multi-layer choropleth map with 6 layers\n",
    "    using dynamic country mappings from the boundary file\n",
    "    \"\"\"\n",
    "    \n",
    "    if not GEOPANDAS_AVAILABLE:\n",
    "        print(\"‚ùå GeoPandas required for choropleth maps\")\n",
    "        return create_fallback_chart(admin_df)\n",
    "    \n",
    "    if not os.path.exists(gpkg_file_path):\n",
    "        print(f\"‚ùå GPKG file not found: {gpkg_file_path}\")\n",
    "        return create_fallback_chart(admin_df)\n",
    "    \n",
    "    if admin_df.empty:\n",
    "        print(\"‚ùå No admin data available\")\n",
    "        return create_fallback_chart(admin_df)\n",
    "    \n",
    "    country_iso = admin_df['ISO'].iloc[0]\n",
    "    country_name = admin_df['Country'].iloc[0]\n",
    "    \n",
    "    print(f\"üó∫Ô∏è Creating DYNAMIC multi-layer choropleth for {country_name}...\")\n",
    "    \n",
    "    # Build dynamic country mappings from boundary file\n",
    "    print(f\"  üìö Building dynamic country mappings...\")\n",
    "    iso_to_gaul, iso_to_names = build_country_mappings_from_boundaries(gpkg_file_path)\n",
    "    \n",
    "    if iso_to_gaul and country_iso in iso_to_gaul:\n",
    "        print(f\"  ‚úÖ Found dynamic mapping: {country_iso} -> GAUL {iso_to_gaul[country_iso]} ({iso_to_names[country_iso][0]})\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è No dynamic mapping found for {country_iso}, will use fallback methods\")\n",
    "    \n",
    "    # Add comprehensive boundary file debugging info\n",
    "    try:\n",
    "        print(f\"  üìÇ Checking boundary file: {gpkg_file_path}\")\n",
    "        available_layers = gpd.list_layers(gpkg_file_path)\n",
    "        if hasattr(available_layers, 'name'):\n",
    "            layer_names = available_layers['name'].tolist()\n",
    "        else:\n",
    "            layer_names = available_layers\n",
    "        print(f\"  üìã Available layers: {layer_names}\")\n",
    "        \n",
    "        # Quick column check for ADM_1\n",
    "        if 'ADM_1' in layer_names:\n",
    "            sample_adm1 = gpd.read_file(gpkg_file_path, layer='ADM_1', rows=1)\n",
    "            print(f\"  üîç Sample ADM_1 columns: {list(sample_adm1.columns)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Could not analyze boundary file: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # STEP 1: Prepare data by admin level\n",
    "        adm1_data = admin_df[admin_df['Admin Level'] == 'ADM1'].copy()\n",
    "        adm2_data = admin_df[admin_df['Admin Level'] == 'ADM2'].copy()\n",
    "        \n",
    "        # Get merged boundary data for each level using DYNAMIC method\n",
    "        merged_datasets = {}\n",
    "        if not adm1_data.empty:\n",
    "            print(f\"  üèõÔ∏è Processing ADM1 data ({len(adm1_data)} records)...\")\n",
    "            adm1_merged = create_merged_boundary_data(adm1_data, 'ADM1', country_iso, country_name, gpkg_file_path)\n",
    "            if adm1_merged is not None and not adm1_merged.empty:\n",
    "                merged_datasets['ADM1'] = adm1_merged\n",
    "                print(f\"  ‚úÖ ADM1 merged successfully\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå ADM1 merge failed\")\n",
    "        \n",
    "        if not adm2_data.empty:\n",
    "            print(f\"  üèõÔ∏è Processing ADM2 data ({len(adm2_data)} records)...\")\n",
    "            adm2_merged = create_merged_boundary_data(adm2_data, 'ADM2', country_iso, country_name, gpkg_file_path)\n",
    "            if adm2_merged is not None and not adm2_merged.empty:\n",
    "                merged_datasets['ADM2'] = adm2_merged\n",
    "                print(f\"  ‚úÖ ADM2 merged successfully\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå ADM2 merge failed\")\n",
    "        \n",
    "        if not merged_datasets:\n",
    "            print(\"‚ùå No merged boundary data available - check GPKG file and country codes\")\n",
    "            print(f\"üí° Country: {country_name} ({country_iso})\")\n",
    "            print(f\"üí° GPKG file: {gpkg_file_path}\")\n",
    "            return create_fallback_chart(admin_df)\n",
    "        \n",
    "        print(f\"  ‚úÖ Successfully merged {len(merged_datasets)} admin levels using DYNAMIC mappings\")\n",
    "        \n",
    "        # Continue with the rest of the choropleth creation (same as before)...\n",
    "        # [Rest of the function remains unchanged - just using the dynamic merged data]\n",
    "        \n",
    "        # STEP 2: Create base map with proper setup\n",
    "        # Calculate center from first available dataset\n",
    "        first_dataset = list(merged_datasets.values())[0]\n",
    "        if first_dataset is None:\n",
    "            print(\"‚ùå First dataset is None\")\n",
    "            return create_fallback_chart(admin_df)\n",
    "        \n",
    "        bounds = first_dataset.total_bounds\n",
    "        center_lat = (bounds[1] + bounds[3]) / 2\n",
    "        center_lon = (bounds[0] + bounds[2]) / 2\n",
    "        \n",
    "        # Calculate zoom\n",
    "        lat_range = bounds[3] - bounds[1]\n",
    "        lon_range = bounds[2] - bounds[0]\n",
    "        max_range = max(lat_range, lon_range)\n",
    "        zoom = 4 if max_range > 15 else 5 if max_range > 8 else 6 if max_range > 4 else 7\n",
    "        \n",
    "        # Initialize map without default tiles\n",
    "        m = folium.Map(\n",
    "            location=[center_lat, center_lon],\n",
    "            zoom_start=zoom,\n",
    "            tiles=None,\n",
    "            width='100%',\n",
    "            height='600px'\n",
    "        )\n",
    "        \n",
    "        # Add base tile layers\n",
    "        folium.TileLayer('OpenStreetMap', name='Street Map', control=True).add_to(m)\n",
    "        folium.TileLayer('CartoDB positron', name='Light Map', control=True).add_to(m)\n",
    "        folium.TileLayer('CartoDB dark_matter', name='Dark Map', control=True).add_to(m)\n",
    "        \n",
    "        print(f\"  ‚úÖ Base map created at ({center_lat:.3f}, {center_lon:.3f}), zoom: {zoom}\")\n",
    "        \n",
    "        # STEP 3: Define layer configurations\n",
    "        layer_configs = []\n",
    "        \n",
    "        # Add ADM1 layers if data exists\n",
    "        if 'ADM1' in merged_datasets:\n",
    "            layer_configs.extend([\n",
    "                {\n",
    "                    'name': 'ADM1 - Deaths',\n",
    "                    'data': merged_datasets['ADM1'],\n",
    "                    'column': 'Total Deaths',\n",
    "                    'color_scheme': 'Reds',\n",
    "                    'show': True,  # Show first layer by default\n",
    "                    'opacity': 0.7\n",
    "                },\n",
    "                {\n",
    "                    'name': 'ADM1 - Affected',\n",
    "                    'data': merged_datasets['ADM1'],\n",
    "                    'column': 'Total Affected',\n",
    "                    'color_scheme': 'Oranges',\n",
    "                    'show': False,\n",
    "                    'opacity': 0.6\n",
    "                },\n",
    "                {\n",
    "                    'name': 'ADM1 - Disasters',\n",
    "                    'data': merged_datasets['ADM1'],\n",
    "                    'column': 'Disaster Count',\n",
    "                    'color_scheme': 'Blues',\n",
    "                    'show': False,\n",
    "                    'opacity': 0.8\n",
    "                }\n",
    "            ])\n",
    "        \n",
    "        # Add ADM2 layers if data exists\n",
    "        if 'ADM2' in merged_datasets:\n",
    "            layer_configs.extend([\n",
    "                {\n",
    "                    'name': 'ADM2 - Deaths',\n",
    "                    'data': merged_datasets['ADM2'],\n",
    "                    'column': 'Total Deaths',\n",
    "                    'color_scheme': 'Reds',\n",
    "                    'show': False,\n",
    "                    'opacity': 0.7\n",
    "                },\n",
    "                {\n",
    "                    'name': 'ADM2 - Affected',\n",
    "                    'data': merged_datasets['ADM2'],\n",
    "                    'column': 'Total Affected',\n",
    "                    'color_scheme': 'Oranges',\n",
    "                    'show': False,\n",
    "                    'opacity': 0.6\n",
    "                },\n",
    "                {\n",
    "                    'name': 'ADM2 - Disasters',\n",
    "                    'data': merged_datasets['ADM2'],\n",
    "                    'column': 'Disaster Count',\n",
    "                    'color_scheme': 'Blues',\n",
    "                    'show': False,\n",
    "                    'opacity': 0.8\n",
    "                }\n",
    "            ])\n",
    "        \n",
    "        # STEP 4: Create FeatureGroups and choropleth layers\n",
    "        layers_created = 0\n",
    "        legend_top_position = 80  # Start position\n",
    "        compact_spacing = 100  # Reduced spacing for compact legends\n",
    "        \n",
    "        for config in layer_configs:\n",
    "            try:\n",
    "                # Check if we have data for this metric\n",
    "                values = config['data'][config['column']].replace(0, np.nan).dropna()\n",
    "                if len(values) == 0:\n",
    "                    print(f\"  ‚ö†Ô∏è No data for {config['name']} - skipping\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"  üé® Creating {config['name']}: {len(values)} areas, range {values.min():.0f}-{values.max():.0f}\")\n",
    "                \n",
    "                # Create FeatureGroup for this layer\n",
    "                fg = folium.FeatureGroup(\n",
    "                    name=config['name'],\n",
    "                    overlay=True,  # Checkbox behavior\n",
    "                    show=config['show']\n",
    "                ).add_to(m)\n",
    "                \n",
    "                # Calculate quantile bins for this specific metric\n",
    "                if len(values) >= 5:\n",
    "                    quantiles = values.quantile([0, 0.2, 0.4, 0.6, 0.8, 1.0]).tolist()\n",
    "                    bins = [0] + [q for q in quantiles if q > 0]\n",
    "                else:\n",
    "                    bins = [0] + sorted(values.unique().tolist())\n",
    "                \n",
    "                bins = sorted(list(set(bins)))  # Remove duplicates\n",
    "                \n",
    "                # Prepare data for choropleth (reset index to avoid conflicts)\n",
    "                layer_data = config['data'].copy()\n",
    "                layer_data = layer_data.reset_index(drop=True)\n",
    "                layer_data['choropleth_id'] = layer_data.index\n",
    "                layer_data[config['column']] = layer_data[config['column']].fillna(0)\n",
    "                \n",
    "                # Create color mapping\n",
    "                color_map = create_color_mapping(layer_data, config['column'], config['color_scheme'], bins)\n",
    "                \n",
    "                # Create GeoJson layer with custom styling\n",
    "                geojson_layer = folium.GeoJson(\n",
    "                    layer_data.to_json(),\n",
    "                    name=config['name'],\n",
    "                    style_function=lambda feature, color_map=color_map: {\n",
    "                        'fillColor': color_map.get(feature['properties']['choropleth_id'], '#gray'),\n",
    "                        'color': 'white',\n",
    "                        'weight': 1,\n",
    "                        'fillOpacity': config['opacity'],\n",
    "                        'opacity': 0.2\n",
    "                    },\n",
    "                    highlight_function=lambda feature: {\n",
    "                        'weight': 3,\n",
    "                        'color': '#666',\n",
    "                        'dashArray': '',\n",
    "                        'fillOpacity': 0.9\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Add GeoJson layer to FeatureGroup\n",
    "                geojson_layer.add_to(fg)\n",
    "                \n",
    "                # Add hover tooltip\n",
    "                folium.GeoJsonTooltip(\n",
    "                    fields=['Admin Unit', config['column']],\n",
    "                    aliases=['Admin Unit:', f\"{config['column']}:\"],\n",
    "                    localize=True,\n",
    "                    sticky=True,\n",
    "                    labels=True,\n",
    "                    style=\"\"\"\n",
    "                        background-color: white;\n",
    "                        border: 2px solid black;\n",
    "                        border-radius: 3px;\n",
    "                        box-shadow: 3px;\n",
    "                        font-size: 12px;\n",
    "                        padding: 10px;\n",
    "                    \"\"\"\n",
    "                ).add_to(geojson_layer)\n",
    "                \n",
    "                # Create custom legend for this layer\n",
    "                add_custom_legend(m, config, bins, legend_top_position, layers_created == 0)\n",
    "                legend_top_position += compact_spacing\n",
    "                \n",
    "                layers_created += 1\n",
    "                print(f\"  ‚úÖ Added {config['name']} with {len(bins)-1} color classes\")\n",
    "                \n",
    "            except Exception as layer_error:\n",
    "                print(f\"  ‚ùå Failed to create {config['name']}: {layer_error}\")\n",
    "                continue\n",
    "        \n",
    "        if layers_created == 0:\n",
    "            print(\"‚ùå No layers could be created\")\n",
    "            return create_fallback_chart(admin_df)\n",
    "        \n",
    "        # STEP 5: Add LayerControl LAST (critical for proper functionality)\n",
    "        folium.LayerControl(\n",
    "            position='topright',\n",
    "            collapsed=False,\n",
    "            autoZIndex=True\n",
    "        ).add_to(m)\n",
    "        \n",
    "        # STEP 6: Add JavaScript for legend visibility control\n",
    "        add_legend_visibility_control(m)\n",
    "        \n",
    "        print(f\"  üéâ SUCCESS: Created {layers_created} choropleth layers with DYNAMIC mappings!\")\n",
    "        return m\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating multi-layer choropleth: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return create_fallback_chart(admin_df)\n",
    "\n",
    "# ============================================================================\n",
    "# FIXED MULTI-LAYER CHOROPLETH IMPLEMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "def create_color_mapping(layer_data, column, color_scheme, bins):\n",
    "    \"\"\"Create color mapping for GeoJson styling\"\"\"\n",
    "    \n",
    "    # Define color schemes\n",
    "    color_schemes = {\n",
    "        'Reds': ['#fee5d9', '#fcbba1', '#fc9272', '#fb6a4a', '#de2d26'],\n",
    "        'Oranges': ['#feedde', '#fdd0a2', '#fdae6b', '#fd8d3c', '#d94701'],\n",
    "        'Blues': ['#eff3ff', '#c6dbef', '#9ecae1', '#6baed6', '#2171b5']\n",
    "    }\n",
    "    \n",
    "    colors = color_schemes.get(color_scheme, color_schemes['Blues'])\n",
    "    \n",
    "    # Create color mapping based on quantile bins\n",
    "    color_map = {}\n",
    "    \n",
    "    for idx, row in layer_data.iterrows():\n",
    "        value = row[column]\n",
    "        choropleth_id = row['choropleth_id']\n",
    "        \n",
    "        # Find which bin this value falls into\n",
    "        color_idx = 0\n",
    "        for i in range(len(bins)-1):\n",
    "            if bins[i] <= value <= bins[i+1]:\n",
    "                color_idx = min(i, len(colors)-1)\n",
    "                break\n",
    "        \n",
    "        color_map[choropleth_id] = colors[color_idx]\n",
    "    \n",
    "    return color_map\n",
    "\n",
    "def add_custom_legend(m, config, bins, top_position, is_visible):\n",
    "    \"\"\"Add a compact custom legend positioned on the left side\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Define color schemes\n",
    "        color_schemes = {\n",
    "            'Reds': ['#fee5d9', '#fcbba1', '#fc9272', '#fb6a4a', '#de2d26'],\n",
    "            'Oranges': ['#feedde', '#fdd0a2', '#fdae6b', '#fd8d3c', '#d94701'],\n",
    "            'Blues': ['#eff3ff', '#c6dbef', '#9ecae1', '#6baed6', '#2171b5']\n",
    "        }\n",
    "        \n",
    "        colors = color_schemes.get(config['color_scheme'], color_schemes['Blues'])\n",
    "        \n",
    "        # Create legend entries\n",
    "        legend_entries = []\n",
    "        for i in range(min(len(bins)-1, len(colors))):\n",
    "            min_val = bins[i]\n",
    "            max_val = bins[i+1]\n",
    "            \n",
    "            # Format labels based on value ranges - MORE COMPACT\n",
    "            if max_val < 1000:\n",
    "                if min_val == 0:\n",
    "                    label = f\"0-{max_val:.0f}\"\n",
    "                else:\n",
    "                    label = f\"{min_val:.0f}-{max_val:.0f}\"\n",
    "            elif max_val < 1000000:\n",
    "                if min_val == 0:\n",
    "                    label = f\"0-{max_val/1000:.1f}k\"\n",
    "                else:\n",
    "                    label = f\"{min_val/1000:.1f}k-{max_val/1000:.1f}k\"\n",
    "            else:\n",
    "                if min_val == 0:\n",
    "                    label = f\"0-{max_val/1000000:.1f}M\"\n",
    "                else:\n",
    "                    label = f\"{min_val/1000000:.1f}M-{max_val/1000000:.1f}M\"\n",
    "            \n",
    "            color_idx = min(i, len(colors)-1)\n",
    "            legend_entries.append((label, colors[color_idx]))\n",
    "        \n",
    "        # Create unique legend ID\n",
    "        legend_id = f\"legend-{config['name'].replace(' ', '-').replace('-', '').lower()}\"\n",
    "        \n",
    "        # Create COMPACT legend HTML with reduced spacing\n",
    "        visibility = 'block' if is_visible else 'none'\n",
    "        legend_html = f\"\"\"\n",
    "        <div id=\"{legend_id}\" \n",
    "             class=\"custom-legend compact-legend\"\n",
    "             style=\"position: fixed; \n",
    "                   left: 15px; \n",
    "                   top: {top_position}px; \n",
    "                   width: 100px;\n",
    "                   background: rgba(255, 255, 255, 0.95); \n",
    "                   border: 1px solid #666;\n",
    "                   border-radius: 4px; \n",
    "                   padding: 6px;\n",
    "                   font-family: Arial, sans-serif;\n",
    "                   font-size: 9px;\n",
    "                   z-index: 1000; \n",
    "                   box-shadow: 0 2px 6px rgba(0,0,0,0.1);\n",
    "                   display: {visibility};\">\n",
    "            <div style=\"font-weight: bold; \n",
    "                       margin-bottom: 4px;\n",
    "                       color: #333; \n",
    "                       font-size: 10px;\n",
    "                       border-bottom: 1px solid #ddd;\n",
    "                       padding-bottom: 2px;\">\n",
    "                {config['name']}\n",
    "            </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add COMPACT legend entries with minimal spacing\n",
    "        for label, color in legend_entries:\n",
    "            legend_html += f\"\"\"\n",
    "            <div style=\"display: flex; \n",
    "                       align-items: center; \n",
    "                       margin-bottom: 2px;\n",
    "                       padding: 0;\">\n",
    "                <div style=\"width: 14px;\n",
    "                           height: 10px;\n",
    "                           background-color: {color}; \n",
    "                           margin-right: 6px;\n",
    "                           border: 1px solid #777;\n",
    "                           border-radius: 1px;\n",
    "                           flex-shrink: 0;\"></div>\n",
    "                <span style=\"font-size: 8px;\n",
    "                            color: #444;\n",
    "                            line-height: 1.0;\">{label}</span>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        legend_html += \"</div>\"\n",
    "        \n",
    "        # Add legend to map\n",
    "        m.get_root().html.add_child(folium.Element(legend_html))\n",
    "        print(f\"    üé® Added compact legend for {config['name']} at position {top_position}px\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ö†Ô∏è Failed to create legend for {config['name']}: {e}\")\n",
    "\n",
    "def add_legend_visibility_control(m):\n",
    "    \"\"\"Add enhanced JavaScript to control legend visibility and dynamic positioning\"\"\"\n",
    "    \n",
    "    legend_control_js = \"\"\"\n",
    "    <script>\n",
    "    // Enhanced legend control with dynamic positioning\n",
    "    function repositionVisibleLegends() {\n",
    "        // Get all custom legends\n",
    "        var allLegends = document.querySelectorAll('.custom-legend');\n",
    "        var visibleLegends = [];\n",
    "        \n",
    "        // Find visible legends and their layer names\n",
    "        var layerLegendMap = {\n",
    "            'ADM1 - Deaths': 'legend-adm1deaths',\n",
    "            'ADM1 - Affected': 'legend-adm1affected', \n",
    "            'ADM1 - Disasters': 'legend-adm1disasters',\n",
    "            'ADM2 - Deaths': 'legend-adm2deaths',\n",
    "            'ADM2 - Affected': 'legend-adm2affected',\n",
    "            'ADM2 - Disasters': 'legend-adm2disasters'\n",
    "        };\n",
    "        \n",
    "        // Check layer control checkboxes to determine which legends should be visible\n",
    "        var layerControl = document.querySelector('.leaflet-control-layers');\n",
    "        if (layerControl) {\n",
    "            var checkboxes = layerControl.querySelectorAll('input[type=\"checkbox\"]');\n",
    "            \n",
    "            checkboxes.forEach(function(checkbox) {\n",
    "                var label = checkbox.nextSibling;\n",
    "                if (label && label.textContent) {\n",
    "                    var layerName = label.textContent.trim();\n",
    "                    var legendId = layerLegendMap[layerName];\n",
    "                    \n",
    "                    if (legendId) {\n",
    "                        var legend = document.getElementById(legendId);\n",
    "                        if (legend) {\n",
    "                            if (checkbox.checked) {\n",
    "                                legend.style.display = 'block';\n",
    "                                visibleLegends.push(legend);\n",
    "                            } else {\n",
    "                                legend.style.display = 'none';\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            });\n",
    "        }\n",
    "        \n",
    "        // Dynamically position visible legends starting from top\n",
    "        var startTop = 80;  // Starting position\n",
    "        var spacing = 100;  // Compact spacing between legends\n",
    "        \n",
    "        visibleLegends.forEach(function(legend, index) {\n",
    "            var newTop = startTop + (index * spacing);\n",
    "            legend.style.top = newTop + 'px';\n",
    "            console.log('Repositioned legend to: ' + newTop + 'px');\n",
    "        });\n",
    "        \n",
    "        console.log('Repositioned ' + visibleLegends.length + ' visible legends');\n",
    "    }\n",
    "    \n",
    "    // Wait for map to fully load\n",
    "    setTimeout(function() {\n",
    "        var layerControl = document.querySelector('.leaflet-control-layers');\n",
    "        if (layerControl) {\n",
    "            var checkboxes = layerControl.querySelectorAll('input[type=\"checkbox\"]');\n",
    "            \n",
    "            // Add event listeners to checkboxes\n",
    "            checkboxes.forEach(function(checkbox) {\n",
    "                checkbox.addEventListener('change', function() {\n",
    "                    // Small delay to ensure DOM is updated\n",
    "                    setTimeout(repositionVisibleLegends, 50);\n",
    "                });\n",
    "            });\n",
    "            \n",
    "            // Initial positioning\n",
    "            repositionVisibleLegends();\n",
    "            \n",
    "            console.log('Enhanced legend visibility and positioning controls initialized');\n",
    "        }\n",
    "    }, 1000);\n",
    "    \n",
    "    // Also reposition on window resize\n",
    "    window.addEventListener('resize', function() {\n",
    "        setTimeout(repositionVisibleLegends, 100);\n",
    "    });\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    \n",
    "    m.get_root().html.add_child(folium.Element(legend_control_js))\n",
    "    print(\"  üîß Added enhanced JavaScript with dynamic legend positioning\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SUBNATIONAL ANALYSIS FUNCTIONS (UNCHANGED - WORKING)\n",
    "# ============================================================================\n",
    "\n",
    "def create_subnational_analysis(df):\n",
    "    \"\"\"Create enhanced subnational analysis for single country\"\"\"\n",
    "    \n",
    "    # Check if Admin Units column exists and has data\n",
    "    if 'Admin Units' not in df.columns:\n",
    "        print(\"No Admin Units column found. Creating summary analysis...\")\n",
    "        return create_country_summary_analysis(df)\n",
    "    \n",
    "    # Parse Admin Units data\n",
    "    admin_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.notna(row.get('Admin Units')):\n",
    "            admin_units = parse_admin_units(row['Admin Units'])\n",
    "            \n",
    "            if admin_units:\n",
    "                for admin_unit in admin_units:\n",
    "                    # Extract admin unit information\n",
    "                    if isinstance(admin_unit, dict):\n",
    "                        # Handle properly formatted admin data\n",
    "                        admin_name = (admin_unit.get('adm1_name') or \n",
    "                                    admin_unit.get('adm2_name') or \n",
    "                                    admin_unit.get('name', 'Unknown'))\n",
    "                        admin_code = (admin_unit.get('adm1_code') or \n",
    "                                    admin_unit.get('adm2_code') or \n",
    "                                    admin_unit.get('code', 'Unknown'))\n",
    "                        admin_level = 'ADM1' if 'adm1_name' in admin_unit else 'ADM2'\n",
    "                    else:\n",
    "                        admin_name = str(admin_unit)\n",
    "                        admin_code = 'Unknown'\n",
    "                        admin_level = 'Unknown'\n",
    "                    \n",
    "                    admin_data.append({\n",
    "                        'Admin Unit': admin_name,\n",
    "                        'Admin Code': admin_code,\n",
    "                        'Admin Level': admin_level,\n",
    "                        'Hazard Type': row['Disaster Type'],\n",
    "                        'Disaster Subtype': row.get('Disaster Subtype', 'N/A'),\n",
    "                        'Year': row['Start Year'],\n",
    "                        'Start Date': row.get('Start Date', 'N/A'),\n",
    "                        'Deaths': row['Total Deaths'],\n",
    "                        'Affected': row['Total Affected'],\n",
    "                        'Homeless': row.get('No Homeless', 0),\n",
    "                        'Damage (000 USD)': row.get('Total Damages (\\'000 US$)', 0),\n",
    "                        'Country': row['Country'],\n",
    "                        'ISO': row['ISO']\n",
    "                    })\n",
    "    \n",
    "    if not admin_data:\n",
    "        print(\"No valid admin units data found. Creating summary analysis...\")\n",
    "        return create_country_summary_analysis(df)\n",
    "    \n",
    "    # Create DataFrame from parsed data\n",
    "    admin_df = pd.DataFrame(admin_data)\n",
    "    \n",
    "    # Create comprehensive subnational analysis\n",
    "    return create_subnational_visualization(admin_df, df)\n",
    "\n",
    "def create_country_summary_analysis(df):\n",
    "    \"\"\"Create summary analysis when no subnational data is available\"\"\"\n",
    "    \n",
    "    # Create summary by year and disaster type\n",
    "    summary = df.groupby(['Start Year', 'Disaster Type']).agg({\n",
    "        'DisNo.': 'count',\n",
    "        'Total Deaths': 'sum',\n",
    "        'Total Affected': 'sum',\n",
    "        'Total Damages (\\'000 US$)': 'sum'\n",
    "    }).reset_index()\n",
    "    summary = summary.sort_values(['Start Year', 'Total Deaths'], ascending=[False, False]).head(30)\n",
    "    \n",
    "    # Create table\n",
    "    fig = go.Figure(data=[\n",
    "        go.Table(\n",
    "            columnwidth=[80, 120, 60, 80, 100, 100],\n",
    "            header=dict(\n",
    "                values=['Year', 'Hazard Type', 'Count', 'Deaths', 'Affected', 'Damage (000 USD)'],\n",
    "                fill_color='lightblue',\n",
    "                align='center',\n",
    "                font=dict(size=12, color='black'),\n",
    "                height=40\n",
    "            ),\n",
    "            cells=dict(\n",
    "                values=[\n",
    "                    summary['Start Year'], \n",
    "                    summary['Disaster Type'],\n",
    "                    summary['DisNo.'], \n",
    "                    summary['Total Deaths'].apply(lambda x: f\"{x:,.0f}\"),\n",
    "                    summary['Total Affected'].apply(lambda x: f\"{x:,.0f}\"),\n",
    "                    summary['Total Damages (\\'000 US$)'].apply(lambda x: f\"{x:,.0f}\")\n",
    "                ],\n",
    "                fill_color='white',\n",
    "                align='center',\n",
    "                font=dict(size=11),\n",
    "                height=30\n",
    "            )\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    country_name = df['Country'].iloc[0] if not df.empty else 'Selected Country'\n",
    "    fig.update_layout(\n",
    "        title=f'Disaster Summary for {country_name} (No Subnational Data Available)',\n",
    "        height=600,\n",
    "        margin=dict(t=80, b=20, l=20, r=20)\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_subnational_visualization(admin_df, original_df):\n",
    "    \"\"\"Create comprehensive subnational visualization with table and charts\"\"\"\n",
    "    \n",
    "    # Get country information\n",
    "    country_name = admin_df['Country'].iloc[0] if not admin_df.empty else 'Selected Country'\n",
    "    country_iso = admin_df['ISO'].iloc[0] if not admin_df.empty else 'XXX'\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=[\n",
    "            'Subnational Disaster Data Table',\n",
    "            '',  # Empty \n",
    "            'Disasters by Administrative Unit',\n",
    "            'Deaths by Administrative Unit',\n",
    "            'Affected Population by Administrative Unit',\n",
    "            'Economic Damage by Administrative Unit'\n",
    "        ],\n",
    "        specs=[\n",
    "            [{\"type\": \"table\", \"colspan\": 2}, None],\n",
    "            [{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "            [{\"type\": \"bar\"}, {\"type\": \"pie\"}]\n",
    "        ],\n",
    "        vertical_spacing=0.08,\n",
    "        horizontal_spacing=0.20\n",
    "    )\n",
    "    \n",
    "    # 1. Create detailed table\n",
    "    table_data = admin_df.sort_values(['Year', 'Deaths', 'Affected'], ascending=[False, False, False]).head(50)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Table(\n",
    "            columnwidth=[100, 80, 100, 60, 60, 80, 80, 80, 100],\n",
    "            header=dict(\n",
    "                values=[\n",
    "                    'Admin Unit', 'Admin Level', 'Hazard Type', 'Year', \n",
    "                    'Deaths', 'Affected', 'Homeless', 'Damage (000 USD)', 'Start Date'\n",
    "                ],\n",
    "                fill_color='lightblue',\n",
    "                align='center',\n",
    "                font=dict(size=11, color='black'),\n",
    "                height=35\n",
    "            ),\n",
    "            cells=dict(\n",
    "                values=[\n",
    "                    table_data['Admin Unit'],\n",
    "                    table_data['Admin Level'],\n",
    "                    table_data['Hazard Type'],\n",
    "                    table_data['Year'],\n",
    "                    table_data['Deaths'].apply(lambda x: f\"{x:,.0f}\"),\n",
    "                    table_data['Affected'].apply(lambda x: f\"{x:,.0f}\"),\n",
    "                    table_data['Homeless'].apply(lambda x: f\"{x:,.0f}\"),\n",
    "                    table_data['Damage (000 USD)'].apply(lambda x: f\"{x:,.0f}\"),\n",
    "                    table_data['Start Date']\n",
    "                ],\n",
    "                fill_color='white',\n",
    "                align='center',\n",
    "                font=dict(size=10),\n",
    "                height=25\n",
    "            )\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Aggregate data for charts\n",
    "    admin_summary = admin_df.groupby('Admin Unit').agg({\n",
    "        'Deaths': 'sum',\n",
    "        'Affected': 'sum',\n",
    "        'Damage (000 USD)': 'sum',\n",
    "        'Year': 'count'  # Count of disasters\n",
    "    }).reset_index()\n",
    "    admin_summary.columns = ['Admin Unit', 'Deaths', 'Affected', 'Damage', 'Disaster Count']\n",
    "    admin_summary = admin_summary.sort_values('Disaster Count', ascending=True).tail(15)  # Top 15\n",
    "    \n",
    "    # 3. Disasters by Admin Unit\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            y=admin_summary['Admin Unit'],\n",
    "            x=admin_summary['Disaster Count'],\n",
    "            orientation='h',\n",
    "            name='Disaster Count',\n",
    "            marker_color='lightblue',\n",
    "            text=admin_summary['Disaster Count'],\n",
    "            textposition='outside'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Deaths by Admin Unit\n",
    "    deaths_data = admin_summary[admin_summary['Deaths'] > 0].sort_values('Deaths', ascending=True).tail(10)\n",
    "    if not deaths_data.empty:\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                y=deaths_data['Admin Unit'],\n",
    "                x=deaths_data['Deaths'],\n",
    "                orientation='h',\n",
    "                name='Deaths',\n",
    "                marker_color='red',\n",
    "                text=deaths_data['Deaths'].apply(lambda x: f\"{x:,.0f}\"),\n",
    "                textposition='outside'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # 5. Affected by Admin Unit\n",
    "    affected_data = admin_summary[admin_summary['Affected'] > 0].sort_values('Affected', ascending=True).tail(10)\n",
    "    if not affected_data.empty:\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                y=affected_data['Admin Unit'],\n",
    "                x=affected_data['Affected'],\n",
    "                orientation='h',\n",
    "                name='Affected',\n",
    "                marker_color='orange',\n",
    "                text=affected_data['Affected'].apply(lambda x: f\"{x:,.0f}\"),\n",
    "                textposition='outside'\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "    \n",
    "    # 6. Hazard types pie chart\n",
    "    hazard_summary = admin_df.groupby('Hazard Type').size().reset_index(name='Count')\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            labels=hazard_summary['Hazard Type'],\n",
    "            values=hazard_summary['Count'],\n",
    "            name='Hazard Types',\n",
    "            textinfo='label+percent'\n",
    "        ),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Subnational Disaster Analysis - {country_name} ({country_iso})',\n",
    "        height=1200,\n",
    "        showlegend=False,\n",
    "        margin=dict(t=120, b=60, l=100, r=100)\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Number of Disasters\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Total Deaths\", row=2, col=2)\n",
    "    fig.update_xaxes(title_text=\"Total Affected\", row=3, col=1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def parse_admin_units_for_choropleth(filtered_df):\n",
    "    \"\"\"Parse admin units data for choropleth mapping - optimized version\"\"\"\n",
    "    \n",
    "    admin_data = []\n",
    "    \n",
    "    try:\n",
    "        for idx, row in filtered_df.iterrows():\n",
    "            if pd.notna(row.get('Admin Units')):\n",
    "                admin_units = parse_admin_units(row['Admin Units'])\n",
    "                \n",
    "                if admin_units:\n",
    "                    for admin_unit in admin_units:\n",
    "                        if isinstance(admin_unit, dict):\n",
    "                            # Handle ADM1 data\n",
    "                            if admin_unit.get('adm1_name') and admin_unit.get('adm1_code'):\n",
    "                                admin_data.append({\n",
    "                                    'Admin Unit': admin_unit['adm1_name'],\n",
    "                                    'Admin Code': admin_unit.get('adm1_code'),\n",
    "                                    'Admin Level': 'ADM1',\n",
    "                                    'Deaths': row.get('Total Deaths', 0),\n",
    "                                    'Affected': row.get('Total Affected', 0),\n",
    "                                    'Damage (000 USD)': row.get('Total Damages (\\'000 US$)', 0),\n",
    "                                    'Year': row.get('Start Year'),\n",
    "                                    'Country': row.get('Country'),\n",
    "                                    'ISO': row.get('ISO')\n",
    "                                })\n",
    "                            \n",
    "                            # Handle ADM2 data\n",
    "                            if admin_unit.get('adm2_name') and admin_unit.get('adm2_code'):\n",
    "                                admin_data.append({\n",
    "                                    'Admin Unit': admin_unit['adm2_name'],\n",
    "                                    'Admin Code': admin_unit.get('adm2_code'),\n",
    "                                    'Admin Level': 'ADM2',\n",
    "                                    'Deaths': row.get('Total Deaths', 0),\n",
    "                                    'Affected': row.get('Total Affected', 0),\n",
    "                                    'Damage (000 USD)': row.get('Total Damages (\\'000 US$)', 0),\n",
    "                                    'Year': row.get('Start Year'),\n",
    "                                    'Country': row.get('Country'),\n",
    "                                    'ISO': row.get('ISO')\n",
    "                                })\n",
    "        \n",
    "        if admin_data:\n",
    "            print(f\"  üìä Parsed {len(admin_data)} admin unit records\")\n",
    "            \n",
    "            # Create summary by admin level\n",
    "            df_temp = pd.DataFrame(admin_data)\n",
    "            level_summary = df_temp.groupby('Admin Level').agg({\n",
    "                'Admin Unit': 'nunique',\n",
    "                'Deaths': 'sum',\n",
    "                'Affected': 'sum'\n",
    "            })\n",
    "            \n",
    "            for level in level_summary.index:\n",
    "                print(f\"    {level}: {level_summary.loc[level, 'Admin Unit']} units, \"\n",
    "                      f\"{level_summary.loc[level, 'Deaths']:.0f} deaths, \"\n",
    "                      f\"{level_summary.loc[level, 'Affected']:.0f} affected\")\n",
    "        \n",
    "        return admin_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error parsing admin units: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_fallback_chart(admin_df):\n",
    "    \"\"\"Create fallback bar chart when choropleth fails\"\"\"\n",
    "    \n",
    "    if admin_df.empty:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        country_name = admin_df['Country'].iloc[0] if 'Country' in admin_df.columns else 'Unknown'\n",
    "        \n",
    "        # Aggregate data by admin unit\n",
    "        admin_summary = admin_df.groupby('Admin Unit').agg({\n",
    "            'Deaths': 'sum',\n",
    "            'Affected': 'sum',\n",
    "            'Year': 'count'\n",
    "        }).reset_index()\n",
    "        admin_summary.columns = ['Admin Unit', 'Total Deaths', 'Total Affected', 'Disaster Count']\n",
    "        admin_summary = admin_summary.sort_values('Disaster Count', ascending=True).tail(15)\n",
    "        \n",
    "        # Create horizontal bar chart\n",
    "        fig = px.bar(\n",
    "            admin_summary,\n",
    "            y='Admin Unit',\n",
    "            x='Disaster Count',\n",
    "            color='Total Deaths',\n",
    "            color_continuous_scale='Reds',\n",
    "            orientation='h',\n",
    "            title=f'{country_name} - Administrative Units Disaster Impact (Choropleth Failed)',\n",
    "            labels={'Disaster Count': 'Number of Disasters', 'Total Deaths': 'Total Deaths'},\n",
    "            text='Disaster Count'\n",
    "        )\n",
    "        \n",
    "        fig.update_traces(textposition='outside')\n",
    "        fig.update_layout(\n",
    "            height=500, \n",
    "            margin=dict(l=150, r=50, t=80, b=50),\n",
    "            title_x=0.5\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating fallback chart: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# ADDITIONAL VISUALIZATION FUNCTIONS (UNCHANGED - WORKING)\n",
    "# ============================================================================\n",
    "\n",
    "def create_overview_stats(df):\n",
    "    \"\"\"Create overview statistics HTML\"\"\"\n",
    "    \n",
    "    total_disasters = len(df)\n",
    "    total_deaths = df['Total Deaths'].sum()\n",
    "    total_affected = df['Total Affected'].sum()\n",
    "    total_damage = df['Total Damages (\\'000 US$)'].sum()\n",
    "    \n",
    "    countries_affected = df['ISO'].nunique()\n",
    "    disaster_types = df['Disaster Type'].nunique()\n",
    "    \n",
    "    html = f\"\"\"\n",
    "    <div style='background-color: #f0f0f0; padding: 20px; border-radius: 10px;'>\n",
    "        <h3>Overview Statistics</h3>\n",
    "        <div style='display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px;'>\n",
    "            <div style='background: white; padding: 15px; border-radius: 5px;'>\n",
    "                <h4 style='color: #e74c3c;'>Total Disasters</h4>\n",
    "                <p style='font-size: 24px; font-weight: bold;'>{total_disasters:,}</p>\n",
    "            </div>\n",
    "            <div style='background: white; padding: 15px; border-radius: 5px;'>\n",
    "                <h4 style='color: #e67e22;'>Total Deaths</h4>\n",
    "                <p style='font-size: 24px; font-weight: bold;'>{total_deaths:,.0f}</p>\n",
    "            </div>\n",
    "            <div style='background: white; padding: 15px; border-radius: 5px;'>\n",
    "                <h4 style='color: #f39c12;'>Total Affected</h4>\n",
    "                <p style='font-size: 24px; font-weight: bold;'>{total_affected:,.0f}</p>\n",
    "            </div>\n",
    "            <div style='background: white; padding: 15px; border-radius: 5px;'>\n",
    "                <h4 style='color: #27ae60;'>Economic Damage</h4>\n",
    "                <p style='font-size: 24px; font-weight: bold;'>${total_damage:,.0f}k</p>\n",
    "            </div>\n",
    "            <div style='background: white; padding: 15px; border-radius: 5px;'>\n",
    "                <h4 style='color: #3498db;'>Countries Affected</h4>\n",
    "                <p style='font-size: 24px; font-weight: bold;'>{countries_affected}</p>\n",
    "            </div>\n",
    "            <div style='background: white; padding: 15px; border-radius: 5px;'>\n",
    "                <h4 style='color: #9b59b6;'>Disaster Types</h4>\n",
    "                <p style='font-size: 24px; font-weight: bold;'>{disaster_types}</p>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return html\n",
    "\n",
    "def create_disaster_type_chart(df):\n",
    "    \"\"\"Create disaster type analysis charts\"\"\"\n",
    "    \n",
    "    # Aggregate data by disaster type\n",
    "    disaster_stats = df.groupby('Disaster Type').agg({\n",
    "        'DisNo.': 'count',\n",
    "        'Total Deaths': 'sum',\n",
    "        'Total Affected': 'sum',\n",
    "        'Total Damages (\\'000 US$)': 'sum'\n",
    "    }).reset_index()\n",
    "    disaster_stats.columns = ['Disaster Type', 'Count', 'Deaths', 'Affected', 'Damage']\n",
    "    disaster_stats = disaster_stats.sort_values('Count', ascending=False).head(15)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Number of Disasters', 'Total Deaths', \n",
    "                       'Total Affected', 'Economic Damage (000 USD)'),\n",
    "        specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
    "               [{'type': 'bar'}, {'type': 'bar'}]]\n",
    "    )\n",
    "    \n",
    "    # Add traces\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=disaster_stats['Disaster Type'], y=disaster_stats['Count'], \n",
    "               name='Count', marker_color='lightblue'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=disaster_stats['Disaster Type'], y=disaster_stats['Deaths'],\n",
    "               name='Deaths', marker_color='red'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=disaster_stats['Disaster Type'], y=disaster_stats['Affected'],\n",
    "               name='Affected', marker_color='orange'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=disaster_stats['Disaster Type'], y=disaster_stats['Damage'],\n",
    "               name='Damage', marker_color='green'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(height=800, showlegend=False, title_text=\"Disaster Impact by Type\")\n",
    "    fig.update_xaxes(tickangle=-45)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_temporal_analysis(df):\n",
    "    \"\"\"Create temporal trend analysis\"\"\"\n",
    "    \n",
    "    # Aggregate by year\n",
    "    yearly_stats = df.groupby('Start Year').agg({\n",
    "        'DisNo.': 'count',\n",
    "        'Total Deaths': 'sum',\n",
    "        'Total Affected': 'sum',\n",
    "        'Disaster Category': lambda x: x.value_counts().to_dict()\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create figure with secondary y-axis\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=('Disaster Frequency Over Time', 'Deaths and Affected Population Over Time'),\n",
    "        specs=[[{\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": True}]]\n",
    "    )\n",
    "    \n",
    "    # Disaster frequency\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=yearly_stats['Start Year'], y=yearly_stats['DisNo.'],\n",
    "                  mode='lines+markers', name='Number of Disasters',\n",
    "                  line=dict(color='blue', width=2)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Deaths and affected\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=yearly_stats['Start Year'], y=yearly_stats['Total Deaths'],\n",
    "                  mode='lines', name='Deaths', line=dict(color='red')),\n",
    "        row=2, col=1, secondary_y=False\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=yearly_stats['Start Year'], y=yearly_stats['Total Affected'],\n",
    "                  mode='lines', name='Affected', line=dict(color='orange')),\n",
    "        row=2, col=1, secondary_y=True\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_xaxes(title_text=\"Year\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Deaths\", secondary_y=False, row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Affected\", secondary_y=True, row=2, col=1)\n",
    "    \n",
    "    fig.update_layout(height=700, title_text=\"Temporal Analysis of Disasters\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_loss_analysis(df):\n",
    "    \"\"\"Create loss analysis by hazard category\"\"\"\n",
    "    \n",
    "    # Aggregate by disaster category\n",
    "    category_stats = df.groupby('Disaster Category').agg({\n",
    "        'Total Deaths': 'sum',\n",
    "        'Total Affected': 'sum',\n",
    "        'Total Damages (\\'000 US$)': 'sum',\n",
    "        'DisNo.': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create pie charts\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Distribution of Disasters', 'Distribution of Deaths',\n",
    "                       'Distribution of Affected', 'Distribution of Economic Damage'),\n",
    "        specs=[[{'type': 'pie'}, {'type': 'pie'}],\n",
    "               [{'type': 'pie'}, {'type': 'pie'}]]\n",
    "    )\n",
    "    \n",
    "    # Add pie charts\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=category_stats['Disaster Category'], \n",
    "               values=category_stats['DisNo.'], name='Count'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=category_stats['Disaster Category'],\n",
    "               values=category_stats['Total Deaths'], name='Deaths'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=category_stats['Disaster Category'],\n",
    "               values=category_stats['Total Affected'], name='Affected'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=category_stats['Disaster Category'],\n",
    "               values=category_stats['Total Damages (\\'000 US$)'], name='Damage'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=700, title_text=\"Loss Distribution by Disaster Category\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_geographic_distribution(df):\n",
    "    \"\"\"Create geographic distribution map\"\"\"\n",
    "    \n",
    "    # Aggregate by country\n",
    "    country_stats = df.groupby(['ISO', 'Country']).agg({\n",
    "        'DisNo.': 'count',\n",
    "        'Total Deaths': 'sum',\n",
    "        'Total Affected': 'sum',\n",
    "        'Total Damages (\\'000 US$)': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create choropleth map\n",
    "    fig = px.choropleth(\n",
    "        country_stats,\n",
    "        locations='ISO',\n",
    "        locationmode='ISO-3',\n",
    "        color='DisNo.',\n",
    "        hover_name='Country',\n",
    "        hover_data={\n",
    "            'DisNo.': ':,',\n",
    "            'Total Deaths': ':,.0f',\n",
    "            'Total Affected': ':,.0f',\n",
    "            'Total Damages (\\'000 US$)': ':,.0f'\n",
    "        },\n",
    "        color_continuous_scale='YlOrRd',\n",
    "        labels={'DisNo.': 'Number of Disasters'},\n",
    "        title='Geographic Distribution of Disasters'\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        geo=dict(\n",
    "            showframe=False,\n",
    "            showcoastlines=True,\n",
    "            projection_type='natural earth'\n",
    "        ),\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# ============================================================================\n",
    "# UPDATED VISUALIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def create_visualizations(filtered_df, dashboard, gpkg_file_path=None):\n",
    "    \"\"\"Create all visualizations for the dashboard with FIXED subnational analysis\"\"\"\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        print(\"No data available for selected filters\")\n",
    "        return\n",
    "    \n",
    "    # Create tabs for different visualizations\n",
    "    tab_contents = []\n",
    "    tab_titles = []\n",
    "    \n",
    "    # Tab 1: Overview Statistics\n",
    "    overview_html = create_overview_stats(filtered_df)\n",
    "    tab_contents.append(widgets.HTML(overview_html))\n",
    "    tab_titles.append('Overview')\n",
    "    \n",
    "    # Tab 2: Disaster Type Analysis\n",
    "    disaster_fig = create_disaster_type_chart(filtered_df)\n",
    "    tab_contents.append(widgets.Output())\n",
    "    with tab_contents[-1]:\n",
    "        disaster_fig.show()\n",
    "    tab_titles.append('Disaster Types')\n",
    "    \n",
    "    # Tab 3: Temporal Analysis\n",
    "    temporal_fig = create_temporal_analysis(filtered_df)\n",
    "    tab_contents.append(widgets.Output())\n",
    "    with tab_contents[-1]:\n",
    "        temporal_fig.show()\n",
    "    tab_titles.append('Temporal Trends')\n",
    "    \n",
    "    # Tab 4: Loss Analysis\n",
    "    loss_fig = create_loss_analysis(filtered_df)\n",
    "    tab_contents.append(widgets.Output())\n",
    "    with tab_contents[-1]:\n",
    "        loss_fig.show()\n",
    "    tab_titles.append('Loss Analysis')\n",
    "    \n",
    "    # Tab 5: Geographic Distribution (if multiple countries)\n",
    "    if filtered_df['ISO'].nunique() > 1:\n",
    "        geo_fig = create_geographic_distribution(filtered_df)\n",
    "        tab_contents.append(widgets.Output())\n",
    "        with tab_contents[-1]:\n",
    "            geo_fig.show()\n",
    "        tab_titles.append('Geographic Distribution')\n",
    "    \n",
    "    # Tab 6: FIXED Subnational Analysis (if single country selected)\n",
    "    if filtered_df['ISO'].nunique() == 1:\n",
    "        # Create the main subnational analysis\n",
    "        subnational_fig = create_subnational_analysis(filtered_df)\n",
    "        if subnational_fig:\n",
    "            tab_contents.append(widgets.Output())\n",
    "            with tab_contents[-1]:\n",
    "                subnational_fig.show()\n",
    "                \n",
    "                # Add FIXED multi-layer choropleth if admin data exists and GPKG provided\n",
    "                if 'Admin Units' in filtered_df.columns and gpkg_file_path and os.path.exists(gpkg_file_path):\n",
    "                    admin_data_count = filtered_df['Admin Units'].notna().sum()\n",
    "                    print(f\"\\nüìä Records with Admin Units: {admin_data_count}\")\n",
    "                    \n",
    "                    if admin_data_count > 0:\n",
    "                        print(f\"\\nüó∫Ô∏è CREATING FIXED MULTI-LAYER CHOROPLETH MAP\")\n",
    "                        print(\"=\"*60)\n",
    "                        \n",
    "                        # Parse admin data for choropleth\n",
    "                        admin_data = parse_admin_units_for_choropleth(filtered_df)\n",
    "                        \n",
    "                        if admin_data:\n",
    "                            admin_df = pd.DataFrame(admin_data)\n",
    "                            print(f\"‚úÖ Parsed {len(admin_df)} admin records\")\n",
    "                            print(f\"üèõÔ∏è Admin levels: {admin_df['Admin Level'].value_counts().to_dict()}\")\n",
    "                            print(f\"üìç Unique admin units: {admin_df['Admin Unit'].nunique()}\")\n",
    "                            \n",
    "                            # Create the FIXED multi-layer choropleth\n",
    "                            choropleth_map = create_working_multi_layer_choropleth(\n",
    "                                admin_df, filtered_df, gpkg_file_path\n",
    "                            )\n",
    "                            \n",
    "                            if choropleth_map:\n",
    "                                print(f\"üéâ SUCCESS: Fixed multi-layer choropleth created!\")\n",
    "                                print(f\"üìã Features:\")\n",
    "                                print(f\"   ‚úÖ 6 independent layers (ADM1/ADM2 √ó Deaths/Affected/Count)\")\n",
    "                                print(f\"   ‚úÖ Proper color scaling (Reds/Oranges/Blues)\")\n",
    "                                print(f\"   ‚úÖ Dynamic legends (250px width, left-side)\")\n",
    "                                print(f\"   ‚úÖ Layer visibility controls\")\n",
    "                                print(f\"   ‚úÖ Hover tooltips and info\")\n",
    "                                display(choropleth_map)\n",
    "                            else:\n",
    "                                print(\"‚ùå Failed to create choropleth map - check data and boundaries\")\n",
    "                        else:\n",
    "                            print(\"‚ùå No valid admin data found after parsing\")\n",
    "                    else:\n",
    "                        print(\"‚ö†Ô∏è No admin units data available for choropleth mapping\")\n",
    "                else:\n",
    "                    if not gpkg_file_path:\n",
    "                        print(\"‚ö†Ô∏è No GPKG file path provided - choropleth mapping disabled\")\n",
    "                    elif not os.path.exists(gpkg_file_path):\n",
    "                        print(f\"‚ö†Ô∏è GPKG file not found: {gpkg_file_path}\")\n",
    "                    else:\n",
    "                        print(\"‚ö†Ô∏è Admin Units column not found in data\")\n",
    "            \n",
    "            tab_titles.append('Subnational Analysis')\n",
    "    \n",
    "    # Create and display tabs\n",
    "    tabs = widgets.Tab(children=tab_contents)\n",
    "    for i, title in enumerate(tab_titles):\n",
    "        tabs.set_title(i, title)\n",
    "    display(tabs)\n",
    "\n",
    "def create_country_selector(dashboard, gpkg_file_path=None):\n",
    "    \"\"\"Create interactive country selector widget with FIXED choropleth\"\"\"\n",
    "    \n",
    "    # Get unique countries with their ISO codes\n",
    "    countries_df = dashboard.df[['Country', 'ISO']].drop_duplicates().sort_values('Country')\n",
    "    country_list = [f\"{row['Country']} ({row['ISO']})\" for _, row in countries_df.iterrows()]\n",
    "    \n",
    "    # Create dropdown widget\n",
    "    country_dropdown = widgets.Dropdown(\n",
    "        options=['All Countries'] + country_list,\n",
    "        value='All Countries',\n",
    "        description='Select Country:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    # Create date range slider\n",
    "    year_range = widgets.IntRangeSlider(\n",
    "        value=[2000, 2024],\n",
    "        min=int(dashboard.df['Start Year'].min()),\n",
    "        max=int(dashboard.df['Start Year'].max()),\n",
    "        step=1,\n",
    "        description='Year Range:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='600px')\n",
    "    )\n",
    "    \n",
    "    # Output area for visualizations\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def update_dashboard(change):\n",
    "        \"\"\"Update dashboard based on selection\"\"\"\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Filter data based on selection\n",
    "            filtered_df = dashboard.df.copy()\n",
    "            \n",
    "            # Filter by year range\n",
    "            filtered_df = filtered_df[\n",
    "                (filtered_df['Start Year'] >= year_range.value[0]) & \n",
    "                (filtered_df['Start Year'] <= year_range.value[1])\n",
    "            ]\n",
    "            \n",
    "            # Filter by country if not \"All Countries\"\n",
    "            if country_dropdown.value != 'All Countries':\n",
    "                country_iso = country_dropdown.value.split('(')[-1].strip(')')\n",
    "                filtered_df = filtered_df[filtered_df['ISO'] == country_iso]\n",
    "                display(HTML(f\"<h2>Disaster Analysis for {country_dropdown.value}</h2>\"))\n",
    "                \n",
    "                # Show choropleth availability info for single country\n",
    "                if gpkg_file_path and 'Admin Units' in filtered_df.columns:\n",
    "                    admin_count = filtered_df['Admin Units'].notna().sum()\n",
    "                    if admin_count > 0:\n",
    "                        coverage = (admin_count / len(filtered_df)) * 100\n",
    "                        display(HTML(f\"<p style='color: green;'><strong>‚úÖ Enhanced choropleth mapping available!</strong><br>\"\n",
    "                                   f\"üìä {admin_count}/{len(filtered_df)} records have subnational data ({coverage:.1f}% coverage)<br>\"\n",
    "                                   f\"üó∫Ô∏è Look for the 'Subnational Analysis' tab with interactive maps</p>\"))\n",
    "                    else:\n",
    "                        display(HTML(f\"<p style='color: orange;'>‚ö†Ô∏è No subnational data available for choropleth mapping</p>\"))\n",
    "            else:\n",
    "                display(HTML(\"<h2>Global Disaster Analysis</h2>\"))\n",
    "            \n",
    "            # Generate visualizations with FIXED choropleth\n",
    "            create_visualizations(filtered_df, dashboard, gpkg_file_path)\n",
    "    \n",
    "    # Link widgets to update function\n",
    "    country_dropdown.observe(update_dashboard, names='value')\n",
    "    year_range.observe(update_dashboard, names='value')\n",
    "    \n",
    "    # Display instructions\n",
    "    instructions_html = \"\"\"\n",
    "    <div style='background-color: #f0f8ff; padding: 15px; border-radius: 8px; margin-bottom: 20px; border-left: 4px solid #007acc;'>\n",
    "        <h3 style='margin-top: 0; color: #007acc;'>üó∫Ô∏è Instructions</h3>\n",
    "        <p><strong>For enhanced mapping:</strong></p>\n",
    "        <ul>\n",
    "            <li>Select a single country with subnational data</li>\n",
    "            <li>Navigate to the 'Subnational Analysis' tab</li>\n",
    "            <li>Use layer checkboxes to toggle between metrics independently</li>\n",
    "            <li>Legends automatically show/hide based on layer visibility</li>\n",
    "            <li>Hover over administrative units for detailed information</li>\n",
    "        </ul>\n",
    "        <p><strong>Available layers:</strong> ADM1/ADM2 levels √ó Deaths (Red) / Affected (Orange) / Disaster Count (Blue)</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Display widgets\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h1>EMDAT Disaster Data Dashboard</h1>\"),\n",
    "        widgets.HTML(instructions_html),\n",
    "        country_dropdown,\n",
    "        year_range,\n",
    "        output\n",
    "    ]))\n",
    "    \n",
    "    # Initial display\n",
    "    update_dashboard(None)\n",
    "\n",
    "def quick_subnational_check(dashboard):\n",
    "    \"\"\"Quick check of subnational data availability across all countries\"\"\"\n",
    "    \n",
    "    countries_with_admin = []\n",
    "    \n",
    "    for country_iso in dashboard.df['ISO'].unique():\n",
    "        country_df = dashboard.df[dashboard.df['ISO'] == country_iso]\n",
    "        country_name = country_df['Country'].iloc[0]\n",
    "        \n",
    "        if 'Admin Units' in country_df.columns:\n",
    "            admin_count = country_df['Admin Units'].notna().sum()\n",
    "            total_count = len(country_df)\n",
    "            \n",
    "            if admin_count > 0:\n",
    "                coverage = (admin_count / total_count) * 100\n",
    "                countries_with_admin.append({\n",
    "                    'Country': country_name,\n",
    "                    'ISO': country_iso,\n",
    "                    'Total_Disasters': total_count,\n",
    "                    'With_Admin_Data': admin_count,\n",
    "                    'Coverage_Percent': coverage\n",
    "                })\n",
    "    \n",
    "    if countries_with_admin:\n",
    "        admin_summary_df = pd.DataFrame(countries_with_admin)\n",
    "        admin_summary_df = admin_summary_df.sort_values('Coverage_Percent', ascending=False)\n",
    "        \n",
    "        print(\"Countries with Subnational Data:\")\n",
    "        print(\"=\"*50)\n",
    "        for _, row in admin_summary_df.head(20).iterrows():\n",
    "            print(f\"{row['Country']} ({row['ISO']}): {row['Coverage_Percent']:.1f}% coverage \"\n",
    "                  f\"({row['With_Admin_Data']}/{row['Total_Disasters']} disasters)\")\n",
    "        \n",
    "        return admin_summary_df\n",
    "    else:\n",
    "        print(\"No countries found with subnational admin data.\")\n",
    "        return None\n",
    "\n",
    "def run_complete_dashboard():\n",
    "    \"\"\"Run the complete EMDAT dashboard with FIXED choropleth\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"EMDAT DISASTER DATA DASHBOARD - FIXED CHOROPLETH\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nInitializing dashboard components...\")\n",
    "    \n",
    "    # Check if data is loaded\n",
    "    if dashboard.df is None:\n",
    "        print(\"Error: No data loaded. Please check your Excel file path.\")\n",
    "        return\n",
    "    \n",
    "    # Check GPKG file availability\n",
    "    if os.path.exists(GPKG_FILE_PATH):\n",
    "        print(f\"‚úÖ GPKG boundaries available: {GPKG_FILE_PATH}\")\n",
    "        print(\"üó∫Ô∏è Enhanced choropleth mapping enabled!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è GPKG file not available - choropleth mapping disabled\")\n",
    "        print(f\"üìÅ Expected path: {GPKG_FILE_PATH}\")\n",
    "    \n",
    "    # Display data quality report\n",
    "    print(\"\\nüìä Data Quality Report:\")\n",
    "    print(f\"  - Total records: {len(dashboard.df):,}\")\n",
    "    print(f\"  - Missing values in key columns:\")\n",
    "    for col in ['Total Deaths', 'Total Affected', 'Total Damages (\\'000 US$)']:\n",
    "        if col in dashboard.df.columns:\n",
    "            missing = dashboard.df[col].isna().sum()\n",
    "            pct = (missing / len(dashboard.df)) * 100\n",
    "            print(f\"    ‚Ä¢ {col}: {missing:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Check subnational data availability\n",
    "    if 'Admin Units' in dashboard.df.columns:\n",
    "        admin_count = dashboard.df['Admin Units'].notna().sum()\n",
    "        admin_pct = (admin_count / len(dashboard.df)) * 100\n",
    "        print(f\"\\nüèõÔ∏è Subnational Data Availability:\")\n",
    "        print(f\"  - Records with admin units: {admin_count:,} ({admin_pct:.1f}%)\")\n",
    "        \n",
    "        if admin_count > 0:\n",
    "            # Show top countries with subnational data\n",
    "            countries_with_admin = []\n",
    "            for country_iso in dashboard.df['ISO'].unique()[:10]:  # Check top 10 countries\n",
    "                country_df = dashboard.df[dashboard.df['ISO'] == country_iso]\n",
    "                country_admin_count = country_df['Admin Units'].notna().sum()\n",
    "                if country_admin_count > 0:\n",
    "                    coverage = (country_admin_count / len(country_df)) * 100\n",
    "                    countries_with_admin.append({\n",
    "                        'Country': country_df['Country'].iloc[0],\n",
    "                        'ISO': country_iso,\n",
    "                        'Coverage': coverage,\n",
    "                        'Records': country_admin_count\n",
    "                    })\n",
    "            \n",
    "            if countries_with_admin:\n",
    "                countries_with_admin.sort(key=lambda x: x['Coverage'], reverse=True)\n",
    "                print(f\"  - Top countries for choropleth mapping:\")\n",
    "                for country in countries_with_admin[:5]:\n",
    "                    print(f\"    ‚Ä¢ {country['Country']} ({country['ISO']}): {country['Coverage']:.1f}% coverage\")\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\nüåç Dataset Summary:\")\n",
    "    print(f\"  - Countries: {dashboard.df['ISO'].nunique()}\")\n",
    "    print(f\"  - Time period: {dashboard.df['Start Year'].min()}-{dashboard.df['Start Year'].max()}\")\n",
    "    print(f\"  - Disaster types: {dashboard.df['Disaster Type'].nunique()}\")\n",
    "    print(f\"  - Total deaths: {dashboard.df['Total Deaths'].sum():,.0f}\")\n",
    "    print(f\"  - Total affected: {dashboard.df['Total Affected'].sum():,.0f}\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Dashboard Features:\")\n",
    "    print(f\"  ‚úÖ Interactive country and time filtering\")\n",
    "    print(f\"  ‚úÖ Multiple visualization tabs\")\n",
    "    print(f\"  ‚úÖ Statistical analysis and charts\") \n",
    "    if os.path.exists(GPKG_FILE_PATH):\n",
    "        print(f\"  ‚úÖ Multi-layer choropleth maps with:\")\n",
    "        print(f\"      ‚Ä¢ 6 independent layers (ADM1/ADM2 √ó 3 metrics)\")\n",
    "        print(f\"      ‚Ä¢ Quantile-based color scaling\")\n",
    "        print(f\"      ‚Ä¢ 250px left-side legends with visibility control\")\n",
    "        print(f\"      ‚Ä¢ Layer checkboxes for independent toggling\")\n",
    "        print(f\"      ‚Ä¢ Hover tooltips and administrative unit info\")\n",
    "    else:\n",
    "        print(f\"  üìä Bar chart fallbacks when choropleth unavailable\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Dashboard ready! Use the interactive controls to explore the data.\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "# Check file availability\n",
    "print(f\"üöÄ INITIALIZING EMDAT DASHBOARD WITH FIXED CHOROPLETH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìÅ Checking file availability...\")\n",
    "if os.path.exists(EXCEL_FILE_PATH):\n",
    "    print(f\"‚úÖ EMDAT Excel file found: {EXCEL_FILE_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ùå EMDAT Excel file not found: {EXCEL_FILE_PATH}\")\n",
    "    print(f\"   Please update EXCEL_FILE_PATH in the configuration above\")\n",
    "\n",
    "if os.path.exists(GPKG_FILE_PATH):\n",
    "    print(f\"‚úÖ GPKG file found: {GPKG_FILE_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ùå GPKG file not found: {GPKG_FILE_PATH}\")\n",
    "    print(f\"   Please update GPKG_FILE_PATH in the configuration above\")\n",
    "    print(f\"   Choropleth maps will not be available without GPKG boundaries\")\n",
    "\n",
    "# Initialize the dashboard\n",
    "print(f\"\\nüèóÔ∏è Initializing EMDAT Dashboard...\")\n",
    "dashboard = EMDATDashboard(EXCEL_FILE_PATH)\n",
    "\n",
    "if dashboard.df is not None:\n",
    "    # Check subnational data availability\n",
    "    print(\"\\nChecking subnational data availability...\")\n",
    "    subnational_summary = quick_subnational_check(dashboard)\n",
    "    \n",
    "    # Run the dashboard\n",
    "    run_complete_dashboard()\n",
    "    \n",
    "    print(f\"\\nüéâ STARTING INTERACTIVE DASHBOARD\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"üéØ Select a country with subnational data to see enhanced choropleth maps!\")\n",
    "    print(\"üìä Use the controls below to filter and explore the data.\")\n",
    "    print(\"\\nüí° Pro tip: Countries with higher subnational coverage will have better choropleth maps.\")\n",
    "    \n",
    "    # Start the interactive dashboard with FIXED implementation\n",
    "    create_country_selector(dashboard, GPKG_FILE_PATH if os.path.exists(GPKG_FILE_PATH) else None)\n",
    "else:\n",
    "    print(\"‚ùå Cannot start dashboard - data not loaded\")\n",
    "    print(\"Please check your EXCEL_FILE_PATH configuration and re-run the script.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
