{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11599218",
   "metadata": {},
   "source": [
    "## HARVESTING AND RESAMPLING OF BUILT-UP DATA FROM WSF-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c073fd71",
   "metadata": {},
   "source": [
    "This script downloads WSF 2019 binary data at 10m for a given country, resample it to 100 m and convert values on a 0 to 1 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40198078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading .tif files: 100%|████████████████████████████████████████████████████████████████████| 4/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mosaicing downloaded .tif files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling WSF 2019 to 100m...\n",
      "Normalising WSF 2019 range [0 to 1]\n",
      "Mosaiced and Upscaled file saved as wd/data/RWA_WSF_2019/RWA_BU.tif\n",
      "Deleted temporary folder: wd/data/RWA_WSF_2019/RWA_tifs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "import shutil\n",
    "from osgeo import gdal\n",
    "from shapely.geometry import shape, MultiPolygon\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely.ops import unary_union\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from gdal_utils import merge_tifs, gdalwarp_wsf19, gdal_calc_wsf19  # Custom functions for geoprocessing\n",
    "import concurrent.futures  # For threading\n",
    "\n",
    "# Define the REST API URL\n",
    "rest_api_url = \"https://services.arcgis.com/iQ1dY19aHwbSDYIF/ArcGIS/rest/services/World_Bank_Global_Administrative_Divisions_VIEW/FeatureServer\"\n",
    "\n",
    "# Function to get the correct layer ID based on administrative level\n",
    "def get_layer_id_for_adm(adm_level):\n",
    "    layers_url = f\"{rest_api_url}/layers\"\n",
    "    response = requests.get(layers_url, params={'f': 'json'})\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        layers_info = response.json().get('layers', [])\n",
    "        target_layer_name = f\"WB_GAD_ADM{adm_level}\"\n",
    "        \n",
    "        for layer in layers_info:\n",
    "            if layer['name'] == target_layer_name:\n",
    "                return layer['id']\n",
    "        \n",
    "        print(f\"Layer matching {target_layer_name} not found.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Failed to fetch layers. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to fetch the ADM data using the correct layer ID\n",
    "def get_adm_data(country, adm_level):\n",
    "    layer_id = get_layer_id_for_adm(adm_level)\n",
    "    \n",
    "    if layer_id is not None:\n",
    "        query_url = f\"{rest_api_url}/{layer_id}/query\"\n",
    "        params = {\n",
    "            'where': f\"ISO_A3 = '{country}'\",\n",
    "            'outFields': '*',\n",
    "            'f': 'geojson'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(query_url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            features = data.get('features', [])\n",
    "            if features:\n",
    "                geometry = [shape(feature['geometry']) for feature in features]\n",
    "                properties = [feature['properties'] for feature in features]\n",
    "                gdf = gpd.GeoDataFrame(properties, geometry=geometry)\n",
    "\n",
    "                return gdf\n",
    "            else:\n",
    "                print(\"No features found for the specified query.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Error fetching data: {response.status_code}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Invalid administrative level or layer mapping not found.\")\n",
    "        return None\n",
    "\n",
    "# Function to download files with progress bar\n",
    "def download_file(url, dest_folder):\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "\n",
    "    local_filename = os.path.join(dest_folder, url.split('/')[-1])\n",
    "\n",
    "    if not os.path.exists(local_filename):\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(local_filename, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "\n",
    "    return local_filename\n",
    "\n",
    "# Mapping of administrative levels to field names\n",
    "adm_field_mapping = {\n",
    "    0: {'code': 'HASC_0', 'name': 'NAM_0'},\n",
    "    1: {'code': 'HASC_1', 'name': 'NAM_1'},\n",
    "    2: {'code': 'HASC_2', 'name': 'NAM_2'},\n",
    "    # Add mappings for additional levels as needed\n",
    "}\n",
    "\n",
    "# SPECIFY COUNTRY TO DOWNLOAD\n",
    "country = 'RWA'  # Country ISO_A3 code\n",
    "adm_level = 2    # Administrative level (e.g., 2)\n",
    "\n",
    "# Fetch the data\n",
    "adm_data = get_adm_data(country, adm_level)\n",
    "\n",
    "if adm_data is not None:\n",
    "    # Step 1: Extract and combine the geometry into a single geometry\n",
    "    ADM_area = adm_data.unary_union  # Combine all geometries into one\n",
    "\n",
    "    if not isinstance(ADM_area, (MultiPolygon, BaseGeometry)):\n",
    "        # If the union results in a non-polygonal shape, handle it accordingly\n",
    "        ADM_area = MultiPolygon([ADM_area])\n",
    "\n",
    "    # Convert to bounding box\n",
    "    bbox = ADM_area.bounds  # (minx, miny, maxx, maxy)\n",
    "\n",
    "    # Step 2: Construct the STAC search query\n",
    "    search_query = {\n",
    "        \"bbox\": list(bbox),  # [minx, miny, maxx, maxy]\n",
    "        \"collections\": [\"WSF_2019\"],  # Assuming this is the correct collection name\n",
    "        \"limit\": 100  # Adjust as needed\n",
    "    }\n",
    "\n",
    "    # Step 3: Send the POST request to the STAC API\n",
    "    stac_search_url = \"https://geoservice.dlr.de/eoc/ogc/stac/v1/search\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(stac_search_url, headers=headers, json=search_query)\n",
    "\n",
    "    # Step 4: Check the response status and parse the results\n",
    "    if response.status_code == 200:\n",
    "        search_results = response.json()\n",
    "        items = search_results.get(\"features\", [])\n",
    "        if items:\n",
    "            print(f\"Found {len(items)} items.\")\n",
    "            # Directory to save downloaded files\n",
    "            subfolder_name = f\"{country}_tifs\"\n",
    "            download_folder = os.path.join(f\"wd/data/{country}_WSF_2019/\", subfolder_name)\n",
    "            if not os.path.exists(download_folder):\n",
    "                os.makedirs(download_folder)\n",
    "\n",
    "            # Step 5: Sequentially download .tif assets into the subdirectory\n",
    "            tif_files = []\n",
    "            total_files = len([asset for item in items for asset in item['assets'].values() if asset['href'].endswith('.tif')])\n",
    "            \n",
    "            with tqdm(total=total_files, desc=\"Downloading .tif files\") as pbar:\n",
    "                for item in items:\n",
    "                    for asset_key, asset_value in item['assets'].items():\n",
    "                        if asset_value['href'].endswith('.tif'):\n",
    "                            tif_file = download_file(asset_value['href'], download_folder)\n",
    "                            tif_files.append(tif_file)\n",
    "                            pbar.update(1)\n",
    "                            \n",
    "            # Step 6: Mosaic the downloaded .tif files using the subdirectory\n",
    "            merged_tif_path = os.path.join(download_folder, f\"{subfolder_name}.tif\")\n",
    "            output_filename = os.path.join(f\"wd/data/{country}_WSF_2019/\", f\"{country}_WSF-2019.tif\")\n",
    "            if tif_files and not os.path.exists(output_filename):\n",
    "                print(\"Mosaicing downloaded .tif files...\")\n",
    "                merge_tifs(download_folder)\n",
    "                os.rename(merged_tif_path, output_filename)\n",
    "            else:\n",
    "                print(\"Mosaic already exists, skipping mosaicing.\")  \n",
    "                \n",
    "            # Step 7: Upscale and normalize the mosaiced file\n",
    "            input_file = output_filename\n",
    "            output_file = os.path.join(f\"wd/data/{country}_WSF_2019/\", f\"{country}_WSF-2019_100m.tif\")\n",
    "            output_calc_file = os.path.join(f\"wd/data/{country}_WSF_2019/\", f\"{country}_BU.tif\")\n",
    "                \n",
    "            if not os.path.exists(output_file):\n",
    "                print(\"Resampling WSF 2019 to 100m...\")\n",
    "                gdalwarp_wsf19(input_file, output_file)\n",
    "            else:\n",
    "                print(f\"{output_file} already exists, skipping upscaling.\")\n",
    "            \n",
    "            if not os.path.exists(output_calc_file):\n",
    "                print(\"Normalising WSF 2019 range [0 to 1]\")\n",
    "                gdal_calc_wsf19(output_file, output_calc_file)\n",
    "                print(f\"Mosaiced and Upscaled file saved as {output_calc_file}\")\n",
    "            else:\n",
    "                print(f\"{output_calc_file} already exists, skipping normalization.\")\n",
    "\n",
    "            # Delete the tiles subfolder after mosaicing\n",
    "            if os.path.exists(download_folder):\n",
    "                shutil.rmtree(download_folder)\n",
    "                print(f\"Deleted temporary folder: {download_folder}\")\n",
    "        else:\n",
    "            print(\"No items found for the specified query.\")\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "else:\n",
    "    print(\"Missing ADM data!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17177b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
